diff -u -r -N driver-master/compcache/Changelog fulan-dvb-drivers-master/compcache/Changelog
--- driver-master/compcache/Changelog	1970-01-01 01:00:00.000000000 +0100
+++ fulan-dvb-drivers-master/compcache/Changelog	2018-04-05 09:41:50.000000000 +0200
@@ -0,0 +1,146 @@
+version 0.7	(TBD)
+ - We can now handle any kind of I/O and not just swap. Device
+   files are also renamed from /dev/ramzswapX to /dev/zramX to
+   reflect this generic nature.
+ - Replaced ioctls with sysfs interface. This also obviates the
+   need for rzscontrol userspace utility.
+ - Removed backing swap support. See: http://lkml.org/lkml/2010/5/13/93
+ - Updated README to reflect above change.
+
+version 0.6.2	(25/1/2010)
+ - Sync-up with mainline version which includes changes below.
+ - Lots and lots of cleanups.
+ - Use small case for ramzswap module parameter: NUM_DEVICES -> num_devices.
+ - Add three module parameters: backing_swap, memlimit_kb and disksize_kb to
+   allow initializing the first device (/dev/ramzswap0) without using
+   rzscontrol utility (see Issue #50).
+ - Use 'struct page' instead of 32-bit PFNs in ramzswap driver and xvmalloc.
+   This is to make these 64-bit safe.
+ - xvmalloc is no longer a separate module and does not export any symbols.
+   Its compiled directly with ramzswap block driver.
+ - Removed useless {load,unload}_modules.sh scripts.
+ - Fix to make sure disksize matches usable pages in backing swap file.
+ - Fix memory leak in (rare) error condition in init_device().
+ - Fix memory leak in (rare) failure in create_device().
+ - Remove swap discard hooks. Swap notifiers makes these redundant.
+ - Unify init_device() fail path and reset_device().
+ - Do not accept backing swap with bad pages.
+ - Fix zero-page accounting.
+ - Use lock for 64-bit stats to prevent value corruption.
+ - Rewrite swap notify patch.
+ - Fix crash when reset is called when there are pending I/Os.
+
+version 0.6	(20/8/2009)
+ - Re-init notify callback to NULL on device reset.
+ - Use block_device to set notify callback.
+ - Proper locking in set_swap_free_notify() callback.
+
+version 0.6pre3	(6/8/2009)
+ - Fix invalid stats reporting on ARM (Issue 34).
+ - Add KERNEL_BUILD_PATH option to Makefile (Issue 35).
+ - Experimental support for 'swap free notification' feature.
+   See README for details.
+ - Patch for Linux kernel 2.6.31-rc5 to support notify feature.
+ - Removed compatibility with kernels older than 2.6.28. This
+   resulted in lot of cleanups.
+
+version 0.6pre2	(21/7/2009)
+ - Fix crashes on ARM (Issue 33).
+ - Close backing swap file on device reset.
+ - Do not change backing swap device block size.
+ - Reset stats on device reset.
+ - Do not allow active device to be reset.
+ - Set ramzswap SSD flag only if backing swap if SSD.
+
+version 0.6pre1	(14/7/2009)
+ - Support for multiple ramzswap devices.
+ - Support for *file* as backing swap.
+ - New rzscontrol utility to control individual
+   ramzswap devices.
+ - Manual page for rzscontrol.
+ - Removed {use,unuse}_compcache.sh scripts.
+ - Above replaced with {load,unload}_modules.sh
+   scripts as all other work is now done with rzscontrol.
+
+version 0.5.3	(8/4/2009)
+ - Major cleanups.
+ - Rename module: compcache.ko -> ramzswap.ko
+ - Rename params: backing_dev -> backing_swap
+ - Updated use_compcache.sh script with detailed
+   documentation on parameters.
+ - LZO de/compress modules are no longer packaged
+   with compcache. Most distros now include these.
+
+version 0.5.2	(11/3/2009)
+ - Can forward incompressible pages to physical swap disk.
+ - New module params:
+ 	- memlimit_kb
+	- disksize_kb
+	- backing_dev
+   See use_compcache.sh for documentation on these params.
+ - Modified use_compcache.sh script to handle new params.
+ - Detect zero-filled pages and don't allocate any memory
+   for them.
+
+version 0.5.1	(22/1/2009)
+ - Fix crash on x86 systems with higmem (mem > ~1G).
+   This required minor changes to atomic (un)map functions (see Issue #20).
+
+version 0.5	(16/1/2009)
+ - Fix crash in case compcache init fails.
+
+version 0.5pre4	(10/1/2009)
+ - Support discarding pages for freed swap blocks (requires 2.6.28-git14).
+   This feature will be disabled if compiled for older kernel.
+ - Mark ramzswap as "solid-state" block device (requires 2.6.26-git14).
+ - Fixed incorrect stats reporting in /proc/compcache (some
+   new stats added too).
+
+version 0.5pre3	(5/1/2009)
+ - Use kmap_atomic() in xvMalloc. This fixes issue #19
+ - Remove xvMapPage() and xvUnmapMap() from xvMalloc.
+
+version 0.5pre2 (28/10/2008)
+ - Alloc full page for uncompressible pages instead
+   of returning I/O error.
+ - Warn users when using ramzswap > (2 x RAM size)
+
+version 0.5pre1 (15/10/2008)
+ - Replaced TLSF with xvMalloc memory allocator
+   http://code.google.com/p/compcache/wiki/xvMalloc
+
+version 0.4	(13/8/2008)
+ - Enable debug and stats option for compcache and tlsf by default
+   proc nodes: /proc/{tlsfinfo,compcache}
+ - Fix crash when reading /proc/tlsfinfo
+ - Lots of cleanups: clean compile on x64
+
+version 0.3	(17/3/2008)
+ - Fix spurious swap read failures
+ - Better swap request filtering
+ - Swap device again renamed to /dev/ramzswap0
+   This is to prevent Ubuntu installer from presenting
+   this device as possible installation target (see Issue #5)
+ - use_compcache.sh script now waits for disk node to be created
+   instead of arbitrary sleep (see Issue #6).
+ - Modified scripts: use_compcache.sh and unuse_compcache.sh
+   to now use new device name (compcache0 -> ramzswap0).
+
+version 0.2	(3/3/2008)
+ - Fixed bug on systems with highmem
+ - Better filtering-out of non-swap requests
+ - Export statistics through proc nodes:
+   - /proc/compcache
+   - /proc/tlsfinfo
+ - Debug and Statistics support for allocator
+   and compcache can now be individually turned
+   on/off by setting DEBUG, STATS to 0/1 in
+   respective header files
+ - Swap device now renamed to /dev/compcache0
+ - Added scripts: use_compcache.sh and unuse_compcache.sh
+   See README for usage
+ - Default compcache size set to 25% of RAM
+ - Lots of code cleanups
+ - Updated README
+ - Created Changelog :)
+
diff -u -r -N driver-master/compcache/compat.h fulan-dvb-drivers-master/compcache/compat.h
--- driver-master/compcache/compat.h	2018-04-16 17:04:38.000000000 +0200
+++ fulan-dvb-drivers-master/compcache/compat.h	2018-04-05 09:41:50.000000000 +0200
@@ -1,23 +1,21 @@
-#ifndef _CCACHE_COMPAT_H_
-#define _CCACHE_COMPAT_H_
+#ifndef _ZRAM_COMPAT_H_
+#define _ZRAM_COMPAT_H_
 
 #include <linux/version.h>
 
-#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
-#define BIO_IO_ERROR(bio)	bio_io_error(bio, PAGE_SIZE)
-#define BIO_ENDIO(bio, error)	bio_endio(bio, PAGE_SIZE, error)
-#else
-#define BIO_IO_ERROR(bio)	bio_io_error(bio)
-#define BIO_ENDIO(bio, error)	bio_endio(bio, error)
-#endif
+/* Uncomment this if you are using swap free notify patch */
+#define CONFIG_SWAP_FREE_NOTIFY
 
-#ifndef pr_info
-#define pr_info(fmt, arg...) \
-	printk(KERN_ERR fmt, ##arg)
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,35))
+#ifndef CONFIG_SWAP_FREE_NOTIFY
+#define CONFIG_SWAP_FREE_NOTIFY
+#endif
 #endif
 
-#ifdef bio_discard
-#define SWAP_DISCARD_SUPPORTED
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2,6,31))
+#define blk_queue_physical_block_size(q, size) \
+	blk_queue_hardsect_size(q, size)
+#define blk_queue_logical_block_size(q, size)
 #endif
 
 #endif
diff -u -r -N driver-master/compcache/lzo-kmod/lzo1x_compress.c fulan-dvb-drivers-master/compcache/lzo-kmod/lzo1x_compress.c
--- driver-master/compcache/lzo-kmod/lzo1x_compress.c	2018-04-16 17:04:38.000000000 +0200
+++ fulan-dvb-drivers-master/compcache/lzo-kmod/lzo1x_compress.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,227 +0,0 @@
-/*
- *  LZO1X Compressor from MiniLZO
- *
- *  Copyright (C) 1996-2005 Markus F.X.J. Oberhumer <markus@oberhumer.com>
- *
- *  The full LZO package can be found at:
- *  http://www.oberhumer.com/opensource/lzo/
- *
- *  Changed for kernel use by:
- *  Nitin Gupta <nitingupta910@gmail.com>
- *  Richard Purdie <rpurdie@openedhand.com>
- */
-
-#include <linux/module.h>
-#include <linux/kernel.h>
-#include <asm/unaligned.h>
-
-#include "lzodefs.h"
-#include "lzo.h"
-
-static noinline size_t
-_lzo1x_1_do_compress(const unsigned char *in, size_t in_len,
-		unsigned char *out, size_t *out_len, void *wrkmem)
-{
-	const unsigned char * const in_end = in + in_len;
-	const unsigned char * const ip_end = in + in_len - M2_MAX_LEN - 5;
-	const unsigned char ** const dict = wrkmem;
-	const unsigned char *ip = in, *ii = ip;
-	const unsigned char *end, *m, *m_pos;
-	size_t m_off, m_len, dindex;
-	unsigned char *op = out;
-
-	ip += 4;
-
-	for (;;) {
-		dindex = ((size_t)(0x21 * DX3(ip, 5, 5, 6)) >> 5) & D_MASK;
-		m_pos = dict[dindex];
-
-		if (m_pos < in)
-			goto literal;
-
-		if (ip == m_pos || ((size_t)(ip - m_pos) > M4_MAX_OFFSET))
-			goto literal;
-
-		m_off = ip - m_pos;
-		if (m_off <= M2_MAX_OFFSET || m_pos[3] == ip[3])
-			goto try_match;
-
-		dindex = (dindex & (D_MASK & 0x7ff)) ^ (D_HIGH | 0x1f);
-		m_pos = dict[dindex];
-
-		if (m_pos < in)
-			goto literal;
-
-		if (ip == m_pos || ((size_t)(ip - m_pos) > M4_MAX_OFFSET))
-			goto literal;
-
-		m_off = ip - m_pos;
-		if (m_off <= M2_MAX_OFFSET || m_pos[3] == ip[3])
-			goto try_match;
-
-		goto literal;
-
-try_match:
-		if (get_unaligned((const unsigned short *)m_pos)
-				== get_unaligned((const unsigned short *)ip)) {
-			if (likely(m_pos[2] == ip[2]))
-					goto match;
-		}
-
-literal:
-		dict[dindex] = ip;
-		++ip;
-		if (unlikely(ip >= ip_end))
-			break;
-		continue;
-
-match:
-		dict[dindex] = ip;
-		if (ip != ii) {
-			size_t t = ip - ii;
-
-			if (t <= 3) {
-				op[-2] |= t;
-			} else if (t <= 18) {
-				*op++ = (t - 3);
-			} else {
-				size_t tt = t - 18;
-
-				*op++ = 0;
-				while (tt > 255) {
-					tt -= 255;
-					*op++ = 0;
-				}
-				*op++ = tt;
-			}
-			do {
-				*op++ = *ii++;
-			} while (--t > 0);
-		}
-
-		ip += 3;
-		if (m_pos[3] != *ip++ || m_pos[4] != *ip++
-				|| m_pos[5] != *ip++ || m_pos[6] != *ip++
-				|| m_pos[7] != *ip++ || m_pos[8] != *ip++) {
-			--ip;
-			m_len = ip - ii;
-
-			if (m_off <= M2_MAX_OFFSET) {
-				m_off -= 1;
-				*op++ = (((m_len - 1) << 5)
-						| ((m_off & 7) << 2));
-				*op++ = (m_off >> 3);
-			} else if (m_off <= M3_MAX_OFFSET) {
-				m_off -= 1;
-				*op++ = (M3_MARKER | (m_len - 2));
-				goto m3_m4_offset;
-			} else {
-				m_off -= 0x4000;
-
-				*op++ = (M4_MARKER | ((m_off & 0x4000) >> 11)
-						| (m_len - 2));
-				goto m3_m4_offset;
-			}
-		} else {
-			end = in_end;
-			m = m_pos + M2_MAX_LEN + 1;
-
-			while (ip < end && *m == *ip) {
-				m++;
-				ip++;
-			}
-			m_len = ip - ii;
-
-			if (m_off <= M3_MAX_OFFSET) {
-				m_off -= 1;
-				if (m_len <= 33) {
-					*op++ = (M3_MARKER | (m_len - 2));
-				} else {
-					m_len -= 33;
-					*op++ = M3_MARKER | 0;
-					goto m3_m4_len;
-				}
-			} else {
-				m_off -= 0x4000;
-				if (m_len <= M4_MAX_LEN) {
-					*op++ = (M4_MARKER
-						| ((m_off & 0x4000) >> 11)
-						| (m_len - 2));
-				} else {
-					m_len -= M4_MAX_LEN;
-					*op++ = (M4_MARKER
-						| ((m_off & 0x4000) >> 11));
-m3_m4_len:
-					while (m_len > 255) {
-						m_len -= 255;
-						*op++ = 0;
-					}
-
-					*op++ = (m_len);
-				}
-			}
-m3_m4_offset:
-			*op++ = ((m_off & 63) << 2);
-			*op++ = (m_off >> 6);
-		}
-
-		ii = ip;
-		if (unlikely(ip >= ip_end))
-			break;
-	}
-
-	*out_len = op - out;
-	return in_end - ii;
-}
-
-int lzo1x_1_compress(const unsigned char *in, size_t in_len, unsigned char *out,
-			size_t *out_len, void *wrkmem)
-{
-	const unsigned char *ii;
-	unsigned char *op = out;
-	size_t t;
-
-	if (unlikely(in_len <= M2_MAX_LEN + 5)) {
-		t = in_len;
-	} else {
-		t = _lzo1x_1_do_compress(in, in_len, op, out_len, wrkmem);
-		op += *out_len;
-	}
-
-	if (t > 0) {
-		ii = in + in_len - t;
-
-		if (op == out && t <= 238) {
-			*op++ = (17 + t);
-		} else if (t <= 3) {
-			op[-2] |= t;
-		} else if (t <= 18) {
-			*op++ = (t - 3);
-		} else {
-			size_t tt = t - 18;
-
-			*op++ = 0;
-			while (tt > 255) {
-				tt -= 255;
-				*op++ = 0;
-			}
-
-			*op++ = tt;
-		}
-		do {
-			*op++ = *ii++;
-		} while (--t > 0);
-	}
-
-	*op++ = M4_MARKER | 1;
-	*op++ = 0;
-	*op++ = 0;
-
-	*out_len = op - out;
-	return LZO_E_OK;
-}
-EXPORT_SYMBOL_GPL(lzo1x_1_compress);
-
-MODULE_LICENSE("GPL");
-MODULE_DESCRIPTION("LZO1X-1 Compressor");
-
diff -u -r -N driver-master/compcache/lzo-kmod/lzo1x_decompress.c fulan-dvb-drivers-master/compcache/lzo-kmod/lzo1x_decompress.c
--- driver-master/compcache/lzo-kmod/lzo1x_decompress.c	2018-04-16 17:04:38.000000000 +0200
+++ fulan-dvb-drivers-master/compcache/lzo-kmod/lzo1x_decompress.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,255 +0,0 @@
-/*
- *  LZO1X Decompressor from MiniLZO
- *
- *  Copyright (C) 1996-2005 Markus F.X.J. Oberhumer <markus@oberhumer.com>
- *
- *  The full LZO package can be found at:
- *  http://www.oberhumer.com/opensource/lzo/
- *
- *  Changed for kernel use by:
- *  Nitin Gupta <nitingupta910@gmail.com>
- *  Richard Purdie <rpurdie@openedhand.com>
- */
-
-#include <linux/module.h>
-#include <linux/kernel.h>
-#include <asm/byteorder.h>
-#include <asm/unaligned.h>
-
-#include "lzodefs.h"
-#include "lzo.h"
-
-#define HAVE_IP(x, ip_end, ip) ((size_t)(ip_end - ip) < (x))
-#define HAVE_OP(x, op_end, op) ((size_t)(op_end - op) < (x))
-#define HAVE_LB(m_pos, out, op) (m_pos < out || m_pos >= op)
-
-#define COPY4(dst, src)	\
-		put_unaligned(get_unaligned((const u32 *)(src)), (u32 *)(dst))
-
-int lzo1x_decompress_safe(const unsigned char *in, size_t in_len,
-			unsigned char *out, size_t *out_len)
-{
-	const unsigned char * const ip_end = in + in_len;
-	unsigned char * const op_end = out + *out_len;
-	const unsigned char *ip = in, *m_pos;
-	unsigned char *op = out;
-	size_t t;
-
-	*out_len = 0;
-
-	if (*ip > 17) {
-		t = *ip++ - 17;
-		if (t < 4)
-			goto match_next;
-		if (HAVE_OP(t, op_end, op))
-			goto output_overrun;
-		if (HAVE_IP(t + 1, ip_end, ip))
-			goto input_overrun;
-		do {
-			*op++ = *ip++;
-		} while (--t > 0);
-		goto first_literal_run;
-	}
-
-	while ((ip < ip_end)) {
-		t = *ip++;
-		if (t >= 16)
-			goto match;
-		if (t == 0) {
-			if (HAVE_IP(1, ip_end, ip))
-				goto input_overrun;
-			while (*ip == 0) {
-				t += 255;
-				ip++;
-				if (HAVE_IP(1, ip_end, ip))
-					goto input_overrun;
-			}
-			t += 15 + *ip++;
-		}
-		if (HAVE_OP(t + 3, op_end, op))
-			goto output_overrun;
-		if (HAVE_IP(t + 4, ip_end, ip))
-			goto input_overrun;
-
-		COPY4(op, ip);
-		op += 4;
-		ip += 4;
-		if (--t > 0) {
-			if (t >= 4) {
-				do {
-					COPY4(op, ip);
-					op += 4;
-					ip += 4;
-					t -= 4;
-				} while (t >= 4);
-				if (t > 0) {
-					do {
-						*op++ = *ip++;
-					} while (--t > 0);
-				}
-			} else {
-				do {
-					*op++ = *ip++;
-				} while (--t > 0);
-			}
-		}
-
-first_literal_run:
-		t = *ip++;
-		if (t >= 16)
-			goto match;
-		m_pos = op - (1 + M2_MAX_OFFSET);
-		m_pos -= t >> 2;
-		m_pos -= *ip++ << 2;
-
-		if (HAVE_LB(m_pos, out, op))
-			goto lookbehind_overrun;
-
-		if (HAVE_OP(3, op_end, op))
-			goto output_overrun;
-		*op++ = *m_pos++;
-		*op++ = *m_pos++;
-		*op++ = *m_pos;
-
-		goto match_done;
-
-		do {
-match:
-			if (t >= 64) {
-				m_pos = op - 1;
-				m_pos -= (t >> 2) & 7;
-				m_pos -= *ip++ << 3;
-				t = (t >> 5) - 1;
-				if (HAVE_LB(m_pos, out, op))
-					goto lookbehind_overrun;
-				if (HAVE_OP(t + 3 - 1, op_end, op))
-					goto output_overrun;
-				goto copy_match;
-			} else if (t >= 32) {
-				t &= 31;
-				if (t == 0) {
-					if (HAVE_IP(1, ip_end, ip))
-						goto input_overrun;
-					while (*ip == 0) {
-						t += 255;
-						ip++;
-						if (HAVE_IP(1, ip_end, ip))
-							goto input_overrun;
-					}
-					t += 31 + *ip++;
-				}
-				m_pos = op - 1;
-				m_pos -= le16_to_cpu(get_unaligned(
-					(const unsigned short *)ip)) >> 2;
-				ip += 2;
-			} else if (t >= 16) {
-				m_pos = op;
-				m_pos -= (t & 8) << 11;
-
-				t &= 7;
-				if (t == 0) {
-					if (HAVE_IP(1, ip_end, ip))
-						goto input_overrun;
-					while (*ip == 0) {
-						t += 255;
-						ip++;
-						if (HAVE_IP(1, ip_end, ip))
-							goto input_overrun;
-					}
-					t += 7 + *ip++;
-				}
-				m_pos -= le16_to_cpu(get_unaligned(
-					(const unsigned short *)ip)) >> 2;
-				ip += 2;
-				if (m_pos == op)
-					goto eof_found;
-				m_pos -= 0x4000;
-			} else {
-				m_pos = op - 1;
-				m_pos -= t >> 2;
-				m_pos -= *ip++ << 2;
-
-				if (HAVE_LB(m_pos, out, op))
-					goto lookbehind_overrun;
-				if (HAVE_OP(2, op_end, op))
-					goto output_overrun;
-
-				*op++ = *m_pos++;
-				*op++ = *m_pos;
-				goto match_done;
-			}
-
-			if (HAVE_LB(m_pos, out, op))
-				goto lookbehind_overrun;
-			if (HAVE_OP(t + 3 - 1, op_end, op))
-				goto output_overrun;
-
-			if (t >= 2 * 4 - (3 - 1) && (op - m_pos) >= 4) {
-				COPY4(op, m_pos);
-				op += 4;
-				m_pos += 4;
-				t -= 4 - (3 - 1);
-				do {
-					COPY4(op, m_pos);
-					op += 4;
-					m_pos += 4;
-					t -= 4;
-				} while (t >= 4);
-				if (t > 0)
-					do {
-						*op++ = *m_pos++;
-					} while (--t > 0);
-			} else {
-copy_match:
-				*op++ = *m_pos++;
-				*op++ = *m_pos++;
-				do {
-					*op++ = *m_pos++;
-				} while (--t > 0);
-			}
-match_done:
-			t = ip[-2] & 3;
-			if (t == 0)
-				break;
-match_next:
-			if (HAVE_OP(t, op_end, op))
-				goto output_overrun;
-			if (HAVE_IP(t + 1, ip_end, ip))
-				goto input_overrun;
-
-			*op++ = *ip++;
-			if (t > 1) {
-				*op++ = *ip++;
-				if (t > 2)
-					*op++ = *ip++;
-			}
-
-			t = *ip++;
-		} while (ip < ip_end);
-	}
-
-	*out_len = op - out;
-	return LZO_E_EOF_NOT_FOUND;
-
-eof_found:
-	*out_len = op - out;
-	return (ip == ip_end ? LZO_E_OK :
-		(ip < ip_end ? LZO_E_INPUT_NOT_CONSUMED : LZO_E_INPUT_OVERRUN));
-input_overrun:
-	*out_len = op - out;
-	return LZO_E_INPUT_OVERRUN;
-
-output_overrun:
-	*out_len = op - out;
-	return LZO_E_OUTPUT_OVERRUN;
-
-lookbehind_overrun:
-	*out_len = op - out;
-	return LZO_E_LOOKBEHIND_OVERRUN;
-}
-
-EXPORT_SYMBOL_GPL(lzo1x_decompress_safe);
-
-MODULE_LICENSE("GPL");
-MODULE_DESCRIPTION("LZO1X Decompressor");
-
diff -u -r -N driver-master/compcache/lzo-kmod/lzodefs.h fulan-dvb-drivers-master/compcache/lzo-kmod/lzodefs.h
--- driver-master/compcache/lzo-kmod/lzodefs.h	2018-04-16 17:04:38.000000000 +0200
+++ fulan-dvb-drivers-master/compcache/lzo-kmod/lzodefs.h	1970-01-01 01:00:00.000000000 +0100
@@ -1,43 +0,0 @@
-/*
- *  lzodefs.h -- architecture, OS and compiler specific defines
- *
- *  Copyright (C) 1996-2005 Markus F.X.J. Oberhumer <markus@oberhumer.com>
- *
- *  The full LZO package can be found at:
- *  http://www.oberhumer.com/opensource/lzo/
- *
- *  Changed for kernel use by:
- *  Nitin Gupta <nitingupta910@gmail.com>
- *  Richard Purdie <rpurdie@openedhand.com>
- */
-
-#define LZO_VERSION		0x2020
-#define LZO_VERSION_STRING	"2.02"
-#define LZO_VERSION_DATE	"Oct 17 2005"
-
-#define M1_MAX_OFFSET	0x0400
-#define M2_MAX_OFFSET	0x0800
-#define M3_MAX_OFFSET	0x4000
-#define M4_MAX_OFFSET	0xbfff
-
-#define M1_MIN_LEN	2
-#define M1_MAX_LEN	2
-#define M2_MIN_LEN	3
-#define M2_MAX_LEN	8
-#define M3_MIN_LEN	3
-#define M3_MAX_LEN	33
-#define M4_MIN_LEN	3
-#define M4_MAX_LEN	9
-
-#define M1_MARKER	0
-#define M2_MARKER	64
-#define M3_MARKER	32
-#define M4_MARKER	16
-
-#define D_BITS		14
-#define D_MASK		((1u << D_BITS) - 1)
-#define D_HIGH		((D_MASK >> 1) + 1)
-
-#define DX2(p, s1, s2)	(((((size_t)((p)[2]) << (s2)) ^ (p)[1]) \
-							<< (s1)) ^ (p)[0])
-#define DX3(p, s1, s2, s3)	((DX2((p)+1, s2, s3) << (s1)) ^ (p)[0])
diff -u -r -N driver-master/compcache/lzo-kmod/lzo.h fulan-dvb-drivers-master/compcache/lzo-kmod/lzo.h
--- driver-master/compcache/lzo-kmod/lzo.h	2018-04-16 17:04:38.000000000 +0200
+++ fulan-dvb-drivers-master/compcache/lzo-kmod/lzo.h	1970-01-01 01:00:00.000000000 +0100
@@ -1,44 +0,0 @@
-#ifndef __LZO_H__
-#define __LZO_H__
-/*
- *  LZO Public Kernel Interface
- *  A mini subset of the LZO real-time data compression library
- *
- *  Copyright (C) 1996-2005 Markus F.X.J. Oberhumer <markus@oberhumer.com>
- *
- *  The full LZO package can be found at:
- *  http://www.oberhumer.com/opensource/lzo/
- *
- *  Changed for kernel use by:
- *  Nitin Gupta <nitingupta910@gmail.com>
- *  Richard Purdie <rpurdie@openedhand.com>
- */
-
-#define LZO1X_MEM_COMPRESS	(16384 * sizeof(unsigned char *))
-#define LZO1X_1_MEM_COMPRESS	LZO1X_MEM_COMPRESS
-
-#define lzo1x_worst_compress(x) ((x) + ((x) / 16) + 64 + 3)
-
-/* This requires 'workmem' of size LZO1X_1_MEM_COMPRESS */
-int lzo1x_1_compress(const unsigned char *src, size_t src_len,
-			unsigned char *dst, size_t *dst_len, void *wrkmem);
-
-/* safe decompression with overrun testing */
-int lzo1x_decompress_safe(const unsigned char *src, size_t src_len,
-			unsigned char *dst, size_t *dst_len);
-
-/*
- * Return values (< 0 = Error)
- */
-#define LZO_E_OK			0
-#define LZO_E_ERROR			(-1)
-#define LZO_E_OUT_OF_MEMORY		(-2)
-#define LZO_E_NOT_COMPRESSIBLE		(-3)
-#define LZO_E_INPUT_OVERRUN		(-4)
-#define LZO_E_OUTPUT_OVERRUN		(-5)
-#define LZO_E_LOOKBEHIND_OVERRUN	(-6)
-#define LZO_E_EOF_NOT_FOUND		(-7)
-#define LZO_E_INPUT_NOT_CONSUMED	(-8)
-#define LZO_E_NOT_YET_IMPLEMENTED	(-9)
-
-#endif
diff -u -r -N driver-master/compcache/lzo-kmod/Makefile fulan-dvb-drivers-master/compcache/lzo-kmod/Makefile
--- driver-master/compcache/lzo-kmod/Makefile	2018-04-16 17:04:38.000000000 +0200
+++ fulan-dvb-drivers-master/compcache/lzo-kmod/Makefile	1970-01-01 01:00:00.000000000 +0100
@@ -1,8 +0,0 @@
-obj-m += lzo1x_compress.o lzo1x_decompress.o
-
-all:
-	make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules
-
-clean:
-	make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean
-
diff -u -r -N driver-master/compcache/Makefile fulan-dvb-drivers-master/compcache/Makefile
--- driver-master/compcache/Makefile	2018-04-16 17:04:38.000000000 +0200
+++ fulan-dvb-drivers-master/compcache/Makefile	2018-04-05 09:41:50.000000000 +0200
@@ -1,14 +1,14 @@
-EXTRA_CFLAGS := -g -Wall
+KERNELDIR ?= "/lib/modules/$(shell uname -r)/build"
 
-obj-m += \
-	lzo-kmod/lzo1x_compress.o \
-	lzo-kmod/lzo1x_decompress.o \
-	ramzswap.o
+XVM = sub-projects/allocators/xvmalloc-kmod
+EXTRA_CFLAGS	:=	-Wall
+
+obj-m		+=	zram.o
+zram-objs	:=	zram_drv.o zram_sysfs.o $(XVM)/xvmalloc.o
 
 all:
-	make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules
+	make -C $(KERNELDIR) M=$(PWD) modules
 
 clean:
-	make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean
+	make -C $(KERNELDIR) M=$(PWD) clean
 	@rm -rf *.ko
-	@rm -rf lzo-kmod/*.ko
diff -u -r -N driver-master/compcache/ramzswap.c fulan-dvb-drivers-master/compcache/ramzswap.c
--- driver-master/compcache/ramzswap.c	2018-04-16 17:04:38.000000000 +0200
+++ fulan-dvb-drivers-master/compcache/ramzswap.c	1970-01-01 01:00:00.000000000 +0100
@@ -1,456 +0,0 @@
-/*
- * Compressed RAM based swap device
- *
- * Copyright (C) 2008, 2009  Nitin Gupta
- *
- * This RAM based block device acts as swap disk.
- * Pages swapped to this device are compressed and
- * stored in memory.
- *
- * Released under the terms of GNU General Public License Version 2.0
- *
- * Project home: http://compcache.googlecode.com
- */
-
-#include <linux/version.h>
-#include <linux/module.h>
-#include <linux/kernel.h>
-#include <linux/bitops.h>
-#include <linux/blkdev.h>
-#include <linux/buffer_head.h>
-#include <linux/device.h>
-#include <linux/genhd.h>
-#include <linux/mutex.h>
-#include <linux/string.h>
-#include <linux/swap.h>
-#include <linux/swapops.h>
-#include <linux/vmalloc.h>
-
-#define BIT(nr) (1UL << (nr))
-
-#include "compat.h"
-#include "ramzswap.h"
-#include "lzo-kmod/lzo.h"
-
-/* Globals */
-static struct ramzswap rzs;
-
-/* Module params (documentation at end) */
-static unsigned long disksize_kb;
-
-static int __init ramzswap_init(void);
-static struct block_device_operations ramzswap_devops = {
-	.owner = THIS_MODULE,
-};
-
-static int test_flag(u32 index, enum rzs_pageflags flag)
-{
-	return rzs.table[index].flags & BIT(flag);
-}
-
-static void set_flag(u32 index, enum rzs_pageflags flag)
-{
-	rzs.table[index].flags |= BIT(flag);
-}
-
-static void clear_flag(u32 index, enum rzs_pageflags flag)
-{
-	rzs.table[index].flags &= ~BIT(flag);
-}
-
-/*
- * Check if request is within bounds and page aligned.
- */
-static inline int valid_swap_request(struct bio *bio)
-{
-	if ( 	(bio->bi_sector >= (rzs.disksize >> SECTOR_SHIFT)) ||
-		(bio->bi_sector & (SECTORS_PER_PAGE - 1)) ||
-		(bio->bi_vcnt != 1) ||
-		(bio->bi_size != PAGE_SIZE) ||
-		(bio->bi_io_vec[0].bv_offset != 0)) {
-
-		return 0;
-	}
-
-	/* swap request is valid */
-	return 1;
-}
-
-/*
- * Called when request page is not present in ramzswap.
- * Its either in backing swap device (if present) or
- * this is an attempt to read before any previous write
- * to this location - this happens due to readahead when
- * swap device is read from user-space (e.g. during swapon)
- */
-int handle_ramzswap_fault(struct bio *bio)
-{
-	struct page *page = bio->bi_io_vec[0].bv_page;
-
-	pr_info(C "Read before write on swap device: "
-		"sector=%lu, size=%u, offset=%u\n",
-		(ulong)(bio->bi_sector), bio->bi_size,
-		bio->bi_io_vec[0].bv_offset);
-	memset(page_address(page), 0, PAGE_SIZE);
-
-	set_bit(BIO_UPTODATE, &bio->bi_flags);
-	BIO_ENDIO(bio, 0);
-	return 0;
-}
-
-int ramzswap_read(struct bio *bio)
-{
-	int ret;
-	u32 index;
-	size_t clen;
-	struct page *page;
-
-	page = bio->bi_io_vec[0].bv_page;
-	index = bio->bi_sector >> SECTORS_PER_PAGE_SHIFT;
-
-	/* Requested page is not present in compressed area */
-	if (!test_flag(index, RZS_INUSE))
-		return handle_ramzswap_fault(bio);
-
-	/* Page is stored uncompressed since its incompressible */
-	if (test_flag(index, RZS_UNCOMPRESSED)) {
-		memcpy(page_address(page), rzs.table[index].pagepos, PAGE_SIZE);
-	} else {
-		clen = PAGE_SIZE;
-
-		ret = lzo1x_decompress_safe(
-			rzs.table[index].pagepos,
-			rzs.table[index].size,
-			page_address(page), &clen);
-
-		if (ret != LZO_E_OK) {
-			pr_info(C "Decompression failed! err=%d, page=%u\n", ret, index);
-			goto out;
-		}
-	}
-	//flush_cache_all(); //ok
-	__flush_purge_region(page_address(page), PAGE_SIZE); //ok
-	__flush_purge_region(rzs.table[index].pagepos, rzs.table[index].size); //ok
-
-	set_bit(BIO_UPTODATE, &bio->bi_flags);
-	BIO_ENDIO(bio, 0);
-	return 0;
-out:
-	BIO_IO_ERROR(bio);
-	return 0;
-}
-
-int ramzswap_write(struct bio *bio)
-{
-	int ret;
-	size_t clen, index;
-	struct page *page;
-	void *lzopage;
-
-	page = bio->bi_io_vec[0].bv_page;
-	index = bio->bi_sector >> SECTORS_PER_PAGE_SHIFT;
-
-	lzopage = rzs.compress_buffer;
-
-	/*
-	 * System swaps to same sector again when the stored page
-	 * is no longer referenced by any process. So, its now safe
-	 * to free the memory that was allocated for this page.
-	 */
-	if (test_flag(index, RZS_INUSE))
-		kfree(rzs.table[index].pagepos);
-
-	clear_flag(index, RZS_INUSE);
-	clear_flag(index, RZS_UNCOMPRESSED);
-
-	mutex_lock(&rzs.lock);
-
-	ret = lzo1x_1_compress(page_address(page), PAGE_SIZE, lzopage, &clen, rzs.compress_workmem);
-
-	if (ret != LZO_E_OK) {
-		pr_info(C "Compression failed! err=%d\n", ret);
-		goto out;
-	}
-
-	/*
-	 * Page is incompressible. Store it as-is (uncompressed)
-	 * since we do not want to return too many swap write
-	 * errors which has side effect of hanging the system.
-	 */
-	if (clen >= PAGE_SIZE) {
-		clen = PAGE_SIZE;
-		set_flag(index, RZS_UNCOMPRESSED);
-		lzopage = page_address(page);
-	}
-
-	rzs.table[index].pagepos = kmalloc(clen, GFP_KERNEL);
-	if (rzs.table[index].pagepos == NULL) {
-		pr_info(C "Error allocating memory");
-		goto out;
-	}
-	memset(rzs.table[index].pagepos, 0, clen);
-
-	set_flag(index, RZS_INUSE);
-	rzs.table[index].size = clen;
-	memcpy(rzs.table[index].pagepos, lzopage, clen);
-
-	mutex_unlock(&rzs.lock);
-
-	set_bit(BIO_UPTODATE, &bio->bi_flags);
-	BIO_ENDIO(bio, 0);
-	return 0;
-out:
-	clear_flag(index, RZS_INUSE);
-	clear_flag(index, RZS_UNCOMPRESSED);
-	mutex_unlock(&rzs.lock);
-	BIO_IO_ERROR(bio);
-	return 0;
-}
-
-/*
- * Handler function for all ramzswap I/O requests.
- */
-static int ramzswap_make_request(struct request_queue *queue, struct bio *bio)
-{
-	int ret = 0;
-
-	if (!valid_swap_request(bio)) {
-		BIO_IO_ERROR(bio);
-		return 0;
-	}
-
-	switch (bio_data_dir(bio)) {
-	case READ:
-		ret = ramzswap_read(bio);
-		break;
-
-	case WRITE:
-		ret = ramzswap_write(bio);
-		break;
-	}
-
-	return ret;
-}
-
-/*
- * Swap header (1st page of swap device) contains information
- * to indentify it as a swap partition. Prepare such a header
- * for ramzswap device (ramzswap0) so that swapon can identify
- * it as swap partition. In case backing swap device is provided,
- * copy its swap header.
- */
-static int setup_swap_header(union swap_header *s)
-{
-	s->info.version = 1;
-	s->info.last_page = rzs.disksize >> PAGE_SHIFT;
-	s->info.nr_badpages = 0;
-	memcpy(s->magic.magic, "SWAPSPACE2", 10);
-	return 0;
-}
-
-static void ramzswap_set_disksize(size_t totalram_bytes)
-{
-	rzs.disksize = disksize_kb << 10;
-
-	if (!disksize_kb) {
-		pr_info(C
-		"disk size not provided. You can use disksize_kb module "
-		"param to specify size.\nUsing default: (%u%% of RAM).\n",
-		DEFAULT_DISKSIZE_PERC_RAM
-		);
-		rzs.disksize = DEFAULT_DISKSIZE_PERC_RAM *
-					(totalram_bytes / 100);
-	}
-
-	if (disksize_kb > 2 * (totalram_bytes >> 10)) {
-		pr_info(C
-		"There is little point creating a ramzswap of greater than "
-		"twice the size of memory since we expect a 2:1 compression "
-		"ratio. Note that ramzswap uses about 0.1%% of the size of "
-		"the swap device when not in use so a huge ramzswap is "
-		"wasteful.\n"
-		"\tMemory Size: %zu kB\n"
-		"\tSize you selected: %lu kB\n"
-		"Continuing anyway ...\n",
-		totalram_bytes >> 10, disksize_kb
-		);
-	}
-
-	rzs.disksize &= PAGE_MASK;
-	pr_info(C "disk size set to %zu kB\n", rzs.disksize >> 10);
-}
-
-static int __init ramzswap_init(void)
-{
-	int ret;
-	size_t num_pages, totalram_bytes;
-	struct sysinfo i;
-	void *page;
-
-	mutex_init(&rzs.lock);
-
-	si_meminfo(&i);
-	/* Here is a trivia: guess unit used for i.totalram !! */
-	totalram_bytes = i.totalram << PAGE_SHIFT;
-
-	ramzswap_set_disksize(totalram_bytes);
-
-	rzs.compress_workmem = kmalloc(LZO1X_MEM_COMPRESS, GFP_KERNEL);
-	if (rzs.compress_workmem == NULL) {
-		pr_info(C "Error allocating compressor working memory\n");
-		ret = -ENOMEM;
-		goto fail;
-	}
-	memset(rzs.compress_workmem, 0, LZO1X_MEM_COMPRESS);
-
-	rzs.compress_buffer = kmalloc(2 * PAGE_SIZE, GFP_KERNEL);
-	if (rzs.compress_buffer == NULL) {
-		pr_info(C "Error allocating compressor buffer space\n");
-		ret = -ENOMEM;
-		goto fail;
-	}
-	memset(rzs.compress_buffer, 0, 2 * PAGE_SIZE);
-
-	num_pages = rzs.disksize >> PAGE_SHIFT;
-	rzs.table = vmalloc(num_pages * sizeof(*rzs.table));
-	if (rzs.table == NULL) {
-		pr_info(C "Error allocating ramzswap address table\n");
-		ret = -ENOMEM;
-		goto fail;
-	}
-	memset(rzs.table, 0, num_pages * sizeof(*rzs.table));
-
-	page = kmalloc(PAGE_SIZE, GFP_KERNEL);
-	if (page == NULL) {
-		pr_info(C "Error allocating swap header page\n");
-		ret = -ENOMEM;
-		goto fail;
-	}
-	memset(page, 0, PAGE_SIZE);
-	rzs.table[0].pagepos = page;
-	set_flag(0, RZS_UNCOMPRESSED);
-	set_flag(0, RZS_INUSE);
-
-	ret = setup_swap_header((union swap_header *)(rzs.table[0].pagepos));
-	if (ret) {
-		pr_info(C "Error setting swap header\n");
-		goto fail;
-	}
-
-	rzs.disk = alloc_disk(1);
-	if (rzs.disk == NULL) {
-		pr_info(C "Error allocating disk structure\n");
-		ret = -ENOMEM;
-		goto fail;
-	}
-
-	rzs.disk->first_minor = 0;
-	rzs.disk->fops = &ramzswap_devops;
-	/*
-	 * It is named like this to prevent distro installers
-	 * from offering ramzswap as installation target. They
-	 * seem to ignore all devices beginning with 'ram'
-	 */
-	strcpy(rzs.disk->disk_name, "ramzswap0");
-
-	rzs.disk->major = register_blkdev(0, rzs.disk->disk_name);
-	if (rzs.disk->major < 0) {
-		pr_info(C "Cannot register block device\n");
-		ret = -EFAULT;
-		goto fail;
-	}
-
-	rzs.disk->queue = blk_alloc_queue(GFP_KERNEL);
-	if (rzs.disk->queue == NULL) {
-		pr_info(C "Cannot register disk queue\n");
-		ret = -EFAULT;
-		goto fail;
-	}
-
-	set_capacity(rzs.disk, rzs.disksize >> SECTOR_SHIFT);
-	blk_queue_make_request(rzs.disk->queue, ramzswap_make_request);
-
-#ifdef QUEUE_FLAG_NONROT
-	/*
-	 * Assuming backing device is "rotational" type.
-	 * TODO: check if its actually "non-rotational" (SSD).
-	 *
-	 * We have ident mapping of sectors for ramzswap and
-	 * and the backing swap device. So, this queue flag
-	 * should be according to backing dev.
-	 */
-	queue_flag_set_unlocked(QUEUE_FLAG_NONROT, rzs.disk->queue);
-#endif
-#ifdef SWAP_DISCARD_SUPPORTED
-	blk_queue_set_discard(rzs.disk->queue, ramzswap_prepare_discard);
-#endif
-#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,30)
-	blk_queue_hardsect_size(rzs.disk->queue, PAGE_SIZE);
-#else
-	blk_queue_logical_block_size(rzs.disk->queue, PAGE_SIZE);
-#endif
-	add_disk(rzs.disk);
-
-	pr_info(C "Initialization done!\n");
-	return 0;
-
-fail:
-	if (rzs.disk != NULL) {
-		if (rzs.disk->major > 0)
-			unregister_blkdev(rzs.disk->major, rzs.disk->disk_name);
-		del_gendisk(rzs.disk);
-	}
-
-	if (test_flag(0, RZS_INUSE))
-		kfree(rzs.table[0].pagepos);
-	kfree(rzs.compress_workmem);
-	kfree(rzs.compress_buffer);
-	vfree(rzs.table);
-
-	pr_info(C "Initialization failed: err=%d\n", ret);
-	return ret;
-}
-
-static void __exit ramzswap_exit(void)
-{
-	size_t index, num_pages;
-	num_pages = rzs.disksize >> PAGE_SHIFT;
-
-	unregister_blkdev(rzs.disk->major, rzs.disk->disk_name);
-	del_gendisk(rzs.disk);
-
-	kfree(rzs.compress_workmem);
-	kfree(rzs.compress_buffer);
-
-	/* Free all pages that are still in ramzswap */
-	for (index = 0; index < num_pages; index++) {
-
-		if (test_flag(index, RZS_INUSE))
-			kfree(rzs.table[index].pagepos);
-	}
-
-	vfree(rzs.table);
-	pr_info(C "cleanup done!\n");
-}
-
-/*
- * This param is applicable only when there is no backing swap device.
- * We ignore this param in case backing dev is provided since then its
- * always equal to size of the backing swap device.
- *
- * This size refers to amount of (uncompressed) data it can hold.
- * For e.g. disksize_kb=1024 means it can hold 1024kb worth of
- * uncompressed data even if this data compresses to just, say, 100kb.
- *
- * Default value is used if this param is missing or 0 (if its applicable).
- * Default: [DEFAULT_DISKSIZE_PERC_RAM]% of RAM
- */
-module_param(disksize_kb, ulong, 0);
-MODULE_PARM_DESC(disksize_kb, "ramzswap device size (kB)");
-
-module_init(ramzswap_init);
-module_exit(ramzswap_exit);
-
-MODULE_LICENSE("GPL");
-MODULE_AUTHOR("Nitin Gupta <ngupta@vflare.org>");
-MODULE_DESCRIPTION("Compressed RAM Based Swap Device");
diff -u -r -N driver-master/compcache/ramzswap.h fulan-dvb-drivers-master/compcache/ramzswap.h
--- driver-master/compcache/ramzswap.h	2018-04-16 17:04:38.000000000 +0200
+++ fulan-dvb-drivers-master/compcache/ramzswap.h	1970-01-01 01:00:00.000000000 +0100
@@ -1,59 +0,0 @@
-/*
- * Compressed RAM based swap device
- *
- * Copyright (C) 2008, 2009  Nitin Gupta
- *
- * This RAM based block device acts as swap disk.
- * Pages swapped to this device are compressed and
- * stored in memory.
- *
- * Released under the terms of GNU General Public License Version 2.0
- *
- * Project home: http://compcache.googlecode.com
- */
-
-#ifndef _RAMZSWAP_H_
-#define _RAMZSWAP_H_
-
-/*-- Configurable parameters */
-
-/* Default ramzswap disk size: 25% of total RAM */
-#define DEFAULT_DISKSIZE_PERC_RAM	25
-#define DEFAULT_MEMLIMIT_PERC_RAM	15
-
-/*-- End of configurable params */
-
-#define SECTOR_SHIFT		9
-#define SECTOR_SIZE		(1 << SECTOR_SHIFT)
-#define SECTORS_PER_PAGE_SHIFT	(PAGE_SHIFT - SECTOR_SHIFT)
-#define SECTORS_PER_PAGE	(1 << SECTORS_PER_PAGE_SHIFT)
-
-/* Message prefix */
-#define C "ramzswap: "
-
-/* Flags for ramzswap pages (table[page_no].flags) */
-enum rzs_pageflags {
-	RZS_UNCOMPRESSED,
-	RZS_INUSE,
-	__NR_RZS_PAGEFLAGS,
-};
-
-/*-- Data structures */
-
-/* Indexed by page no. */
-struct table {
-	void *pagepos;
-	u8 flags;
-	size_t size;
-};
-
-struct ramzswap {
-	void *compress_workmem;
-	void *compress_buffer;
-	struct table *table;
-	struct mutex lock;
-	struct gendisk *disk;
-	size_t disksize;	/* bytes */
-};
-
-#endif
diff -u -r -N driver-master/compcache/README fulan-dvb-drivers-master/compcache/README
--- driver-master/compcache/README	1970-01-01 01:00:00.000000000 +0100
+++ fulan-dvb-drivers-master/compcache/README	2018-04-05 09:41:50.000000000 +0200
@@ -0,0 +1,105 @@
+zram: Compressed RAM based block devices
+----------------------------------------
+
+Project home: http://compcache.googlecode.com/
+
+* Introduction
+
+The zram module creates RAM based block devices named /dev/zram<id>
+(<id> = 0, 1, ...). Pages written to these disks are compressed and stored
+in memory itself. These disks allow very fast I/O and compression provides
+good amounts of memory savings. Some of the usecases include /tmp storage,
+use as swap disks, various caches under /var and maybe many more :)
+
+Statistics for individual zram devices are exported through sysfs nodes at
+/sys/block/zram<id>/
+
+Help page:
+  http://compcache.googlecode.com/CompilingAndUsingNew
+
+* Compiling
+  - 'make':	This will compile zram module against your kernel
+  - Optional (HIGHLY RECOMMENDED): ** Only for kernels older than 2.6.35 **
+	- Apply the patch found in compcache/patches/ directory and just
+	  compile the kernel as usual. This patch enables 'swap free notify'
+	  feature which allows the kernel to send callback to zram as soon
+	  as a swap slot becomes free. So, we can immediately free memory
+	  allocated for the corresponding page, eliminating any stale data
+	  in (compressed) memory. Currently, patches are available for
+	  2.6.29 and 2.6.33 kernels but it should apply to 'nearby' kernel
+	  versions too.  
+	- Uncomment '#define CONFIG_SWAP_FREE_NOTIFY' in compcache/compat.h
+	  before compiling compcache against this patched kernel. Otherwise,
+          this swap notify callback will not be used.
+	- Note that this patch is applicable only when zram devices are
+	  intended to be used as swap disks. They do not help when used
+	  for other purposes (see examples below).
+
+Following binaries are created:
+  - zram.ko (kernel driver)
+
+* Usage
+
+Following shows a typical sequence of steps for using zram.
+
+1) Load Module:
+	modprobe zram num_devices=4
+	This creates 4 devices: /dev/zram{0,1,2,3}
+	(num_devices parameter is optional. Default: 1)
+
+2) Set Disksize:
+	Set disk size by writing the value to sysfs node 'disksize'
+	(in bytes). If disksize is not given, default value of 25%
+	of RAM is used.
+
+	# Set disksize of 50MB for /dev/zram0
+	echo $((50*1024*1024)) > /sys/block/zram0/disksize
+
+	NOTE: disksize cannot be changed if the disk contains any
+	data. So, for such a disk, you need to issue 'reset' (see below)
+	before you can change its disksize.
+
+3) Activate:
+	mkswap /dev/zram0
+	swapon /dev/zram0
+
+	mkfs.ext4 /dev/zram1
+	mount /dev/zram1 /tmp
+
+4) Statistics:
+	Per-device statistics are exported as various nodes under
+	/sys/block/zram<id>/
+		disksize
+		num_reads
+		num_writes
+		invalid_io
+		notify_free
+		discard
+		zero_pages
+		orig_data_size
+		compr_data_size
+		mem_used_total
+
+	A helper script is included (sub-projects/scripts/zram_stats)
+	which shows these stats for devices containing any data. It also
+	shows (derived) values for average compression ratio and memory
+	overhead.
+
+5) Deactivate:
+	swapoff /dev/zram0
+	umount /dev/zram1
+
+6) Reset:
+	Write any positive value to 'reset' sysfs node
+	echo 1 > /sys/block/zram0/reset
+	echo 1 > /sys/block/zram1/reset
+
+	(This frees all the memory allocated for the given device).
+
+
+Please report any problems at:
+ - Mailing list: linux-mm-cc at laptop dot org
+ - Issue tracker: http://code.google.com/p/compcache/issues/list
+
+Nitin Gupta
+ngupta@vflare.org
diff -u -r -N driver-master/compcache/sub-projects/allocators/xvmalloc-kmod/Makefile fulan-dvb-drivers-master/compcache/sub-projects/allocators/xvmalloc-kmod/Makefile
--- driver-master/compcache/sub-projects/allocators/xvmalloc-kmod/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ fulan-dvb-drivers-master/compcache/sub-projects/allocators/xvmalloc-kmod/Makefile	2018-04-05 09:41:50.000000000 +0200
@@ -0,0 +1,12 @@
+EXTRA_CFLAGS	:=	-g -O2 -Wall
+KERNEL_BUILD_PATH ?= "/lib/modules/$(shell uname -r)/build"
+
+obj-m		+=	xvmalloc.o
+
+all:
+	make -C $(KERNEL_BUILD_PATH) M=$(PWD) modules
+
+clean:
+	make -C $(KERNEL_BUILD_PATH) M=$(PWD) clean
+	@$(RM) -rf *.o *~ *.c.gcov *.gcda *.gcno cscope.* tags
+
diff -u -r -N driver-master/compcache/sub-projects/allocators/xvmalloc-kmod/xvmalloc.c fulan-dvb-drivers-master/compcache/sub-projects/allocators/xvmalloc-kmod/xvmalloc.c
--- driver-master/compcache/sub-projects/allocators/xvmalloc-kmod/xvmalloc.c	1970-01-01 01:00:00.000000000 +0100
+++ fulan-dvb-drivers-master/compcache/sub-projects/allocators/xvmalloc-kmod/xvmalloc.c	2018-05-16 23:58:02.742508437 +0200
@@ -0,0 +1,498 @@
+/*
+ * xvmalloc memory allocator
+ *
+ * Copyright (C) 2008, 2009, 2010  Nitin Gupta
+ *
+ * This code is released using a dual license strategy: BSD/GPL
+ * You can choose the licence that better fits your requirements.
+ *
+ * Released under the terms of 3-clause BSD License
+ * Released under the terms of GNU General Public License Version 2.0
+ */
+
+#include <linux/bitops.h>
+#include <linux/errno.h>
+#include <linux/highmem.h>
+#include <linux/init.h>
+#include <linux/string.h>
+#include <linux/slab.h>
+
+#include "xvmalloc.h"
+#include "xvmalloc_int.h"
+
+static void stat_inc(u64 *value)
+{
+	*value = *value + 1;
+}
+
+static void stat_dec(u64 *value)
+{
+	*value = *value - 1;
+}
+
+static int test_flag(struct block_header *block, enum blockflags flag)
+{
+	return block->prev & BIT(flag);
+}
+
+static void set_flag(struct block_header *block, enum blockflags flag)
+{
+	block->prev |= BIT(flag);
+}
+
+static void clear_flag(struct block_header *block, enum blockflags flag)
+{
+	block->prev &= ~BIT(flag);
+}
+
+/*
+ * Given <page, offset> pair, provide a dereferencable pointer.
+ * This is called from xv_malloc/xv_free path, so it
+ * needs to be fast.
+ */
+static void *get_ptr_atomic(struct page *page, u16 offset, enum km_type type)
+{
+	unsigned char *base;
+
+	base = kmap_atomic(page, type);
+	return base + offset;
+}
+
+static void put_ptr_atomic(void *ptr, enum km_type type)
+{
+	kunmap_atomic(ptr, type);
+}
+
+static u32 get_blockprev(struct block_header *block)
+{
+	return block->prev & PREV_MASK;
+}
+
+static void set_blockprev(struct block_header *block, u16 new_offset)
+{
+	block->prev = new_offset | (block->prev & FLAGS_MASK);
+}
+
+static struct block_header *BLOCK_NEXT(struct block_header *block)
+{
+	return (struct block_header *)
+		((char *)block + block->size + XV_ALIGN);
+}
+
+/*
+ * Get index of free list containing blocks of maximum size
+ * which is less than or equal to given size.
+ */
+static u32 get_index_for_insert(u32 size)
+{
+	if (unlikely(size > XV_MAX_ALLOC_SIZE))
+		size = XV_MAX_ALLOC_SIZE;
+	size &= ~FL_DELTA_MASK;
+	return (size - XV_MIN_ALLOC_SIZE) >> FL_DELTA_SHIFT;
+}
+
+/*
+ * Get index of free list having blocks of size greater than
+ * or equal to requested size.
+ */
+static u32 get_index(u32 size)
+{
+	if (unlikely(size < XV_MIN_ALLOC_SIZE))
+		size = XV_MIN_ALLOC_SIZE;
+	size = ALIGN(size, FL_DELTA);
+	return (size - XV_MIN_ALLOC_SIZE) >> FL_DELTA_SHIFT;
+}
+
+/**
+ * find_block - find block of at least given size
+ * @pool: memory pool to search from
+ * @size: size of block required
+ * @page: page containing required block
+ * @offset: offset within the page where block is located.
+ *
+ * Searches two level bitmap to locate block of at least
+ * the given size. If such a block is found, it provides
+ * <page, offset> to identify this block and returns index
+ * in freelist where we found this block.
+ * Otherwise, returns 0 and <page, offset> params are not touched.
+ */
+static u32 find_block(struct xv_pool *pool, u32 size,
+			struct page **page, u32 *offset)
+{
+	ulong flbitmap, slbitmap;
+	u32 flindex, slindex, slbitstart;
+
+	/* There are no free blocks in this pool */
+	if (!pool->flbitmap)
+		return 0;
+
+	/* Get freelist index correspoding to this size */
+	slindex = get_index(size);
+	slbitmap = pool->slbitmap[slindex / BITS_PER_LONG];
+	slbitstart = slindex % BITS_PER_LONG;
+
+	/*
+	 * If freelist is not empty at this index, we found the
+	 * block - head of this list. This is approximate best-fit match.
+	 */
+	if (test_bit(slbitstart, &slbitmap)) {
+		*page = pool->freelist[slindex].page;
+		*offset = pool->freelist[slindex].offset;
+		return slindex;
+	}
+
+	/*
+	 * No best-fit found. Search a bit further in bitmap for a free block.
+	 * Second level bitmap consists of series of 32-bit chunks. Search
+	 * further in the chunk where we expected a best-fit, starting from
+	 * index location found above.
+	 */
+	slbitstart++;
+	slbitmap >>= slbitstart;
+
+	/* Skip this search if we were already at end of this bitmap chunk */
+	if ((slbitstart != BITS_PER_LONG) && slbitmap) {
+		slindex += __ffs(slbitmap) + 1;
+		*page = pool->freelist[slindex].page;
+		*offset = pool->freelist[slindex].offset;
+		return slindex;
+	}
+
+	/* Now do a full two-level bitmap search to find next nearest fit */
+	flindex = slindex / BITS_PER_LONG;
+
+	flbitmap = (pool->flbitmap) >> (flindex + 1);
+	if (!flbitmap)
+		return 0;
+
+	flindex += __ffs(flbitmap) + 1;
+	slbitmap = pool->slbitmap[flindex];
+	slindex = (flindex * BITS_PER_LONG) + __ffs(slbitmap);
+	*page = pool->freelist[slindex].page;
+	*offset = pool->freelist[slindex].offset;
+
+	return slindex;
+}
+
+/*
+ * Insert block at <page, offset> in freelist of given pool.
+ * freelist used depends on block size.
+ */
+static void insert_block(struct xv_pool *pool, struct page *page, u32 offset,
+			struct block_header *block)
+{
+	u32 flindex, slindex;
+	struct block_header *nextblock;
+
+	slindex = get_index_for_insert(block->size);
+	flindex = slindex / BITS_PER_LONG;
+
+	block->link.prev_page = NULL;
+	block->link.prev_offset = 0;
+	block->link.next_page = pool->freelist[slindex].page;
+	block->link.next_offset = pool->freelist[slindex].offset;
+	pool->freelist[slindex].page = page;
+	pool->freelist[slindex].offset = offset;
+
+	if (block->link.next_page) {
+		nextblock = get_ptr_atomic(block->link.next_page,
+					block->link.next_offset, KM_USER1);
+		nextblock->link.prev_page = page;
+		nextblock->link.prev_offset = offset;
+		put_ptr_atomic(nextblock, KM_USER1);
+		/* If there was a next page then the free bits are set. */
+		return;
+	}
+
+	__set_bit(slindex % BITS_PER_LONG, &pool->slbitmap[flindex]);
+	__set_bit(flindex, &pool->flbitmap);
+}
+
+/*
+ * Remove block from freelist. Index 'slindex' identifies the freelist.
+ */
+static void remove_block(struct xv_pool *pool, struct page *page, u32 offset,
+			struct block_header *block, u32 slindex)
+{
+	u32 flindex = slindex / BITS_PER_LONG;
+	struct block_header *tmpblock;
+
+	if (block->link.prev_page) {
+		tmpblock = get_ptr_atomic(block->link.prev_page,
+				block->link.prev_offset, KM_USER1);
+		tmpblock->link.next_page = block->link.next_page;
+		tmpblock->link.next_offset = block->link.next_offset;
+		put_ptr_atomic(tmpblock, KM_USER1);
+	}
+
+	if (block->link.next_page) {
+		tmpblock = get_ptr_atomic(block->link.next_page,
+				block->link.next_offset, KM_USER1);
+		tmpblock->link.prev_page = block->link.prev_page;
+		tmpblock->link.prev_offset = block->link.prev_offset;
+		put_ptr_atomic(tmpblock, KM_USER1);
+	}
+
+	/* Is this block is at the head of the freelist? */
+	if (pool->freelist[slindex].page == page
+	   && pool->freelist[slindex].offset == offset) {
+
+		pool->freelist[slindex].page = block->link.next_page;
+		pool->freelist[slindex].offset = block->link.next_offset;
+
+		if (pool->freelist[slindex].page) {
+			struct block_header *tmpblock;
+			tmpblock = get_ptr_atomic(pool->freelist[slindex].page,
+					pool->freelist[slindex].offset,
+					KM_USER1);
+			tmpblock->link.prev_page = NULL;
+			tmpblock->link.prev_offset = 0;
+			put_ptr_atomic(tmpblock, KM_USER1);
+		} else {
+			/* This freelist bucket is empty */
+			__clear_bit(slindex % BITS_PER_LONG,
+				    &pool->slbitmap[flindex]);
+			if (!pool->slbitmap[flindex])
+				__clear_bit(flindex, &pool->flbitmap);
+		}
+	}
+
+	block->link.prev_page = NULL;
+	block->link.prev_offset = 0;
+	block->link.next_page = NULL;
+	block->link.next_offset = 0;
+}
+
+/*
+ * Allocate a page and add it to freelist of given pool.
+ */
+static int grow_pool(struct xv_pool *pool, gfp_t flags)
+{
+	struct page *page;
+	struct block_header *block;
+
+	page = alloc_page(flags);
+	if (unlikely(!page))
+		return -ENOMEM;
+
+	stat_inc(&pool->total_pages);
+
+	spin_lock(&pool->lock);
+	block = get_ptr_atomic(page, 0, KM_USER0);
+
+	block->size = PAGE_SIZE - XV_ALIGN;
+	set_flag(block, BLOCK_FREE);
+	clear_flag(block, PREV_FREE);
+	set_blockprev(block, 0);
+
+	insert_block(pool, page, 0, block);
+
+	put_ptr_atomic(block, KM_USER0);
+	spin_unlock(&pool->lock);
+
+	return 0;
+}
+
+/*
+ * Create a memory pool. Allocates freelist, bitmaps and other
+ * per-pool metadata.
+ */
+struct xv_pool *xv_create_pool(void)
+{
+	u32 ovhd_size;
+	struct xv_pool *pool;
+
+	ovhd_size = roundup(sizeof(*pool), PAGE_SIZE);
+	pool = kzalloc(ovhd_size, GFP_KERNEL);
+	if (!pool)
+		return NULL;
+
+	spin_lock_init(&pool->lock);
+
+	return pool;
+}
+
+void xv_destroy_pool(struct xv_pool *pool)
+{
+	kfree(pool);
+}
+
+/**
+ * xv_malloc - Allocate block of given size from pool.
+ * @pool: pool to allocate from
+ * @size: size of block to allocate
+ * @page: page no. that holds the object
+ * @offset: location of object within page
+ *
+ * On success, <page, offset> identifies block allocated
+ * and 0 is returned. On failure, <page, offset> is set to
+ * 0 and -ENOMEM is returned.
+ *
+ * Allocation requests with size > XV_MAX_ALLOC_SIZE will fail.
+ */
+int xv_malloc(struct xv_pool *pool, u32 size, struct page **page,
+		u32 *offset, gfp_t flags)
+{
+	int error;
+	u32 index, tmpsize, origsize, tmpoffset;
+	struct block_header *block, *tmpblock;
+
+	*page = NULL;
+	*offset = 0;
+	origsize = size;
+
+	if (unlikely(!size || size > XV_MAX_ALLOC_SIZE))
+		return -ENOMEM;
+
+	size = ALIGN(size, XV_ALIGN);
+
+	spin_lock(&pool->lock);
+
+	index = find_block(pool, size, page, offset);
+
+	if (!*page) {
+		spin_unlock(&pool->lock);
+		if (flags & GFP_NOWAIT)
+			return -ENOMEM;
+		error = grow_pool(pool, flags);
+		if (unlikely(error))
+			return error;
+
+		spin_lock(&pool->lock);
+		index = find_block(pool, size, page, offset);
+	}
+
+	if (!*page) {
+		spin_unlock(&pool->lock);
+		return -ENOMEM;
+	}
+
+	block = get_ptr_atomic(*page, *offset, KM_USER0);
+
+	remove_block(pool, *page, *offset, block, index);
+
+	/* Split the block if required */
+	tmpoffset = *offset + size + XV_ALIGN;
+	tmpsize = block->size - size;
+	tmpblock = (struct block_header *)((char *)block + size + XV_ALIGN);
+	if (tmpsize) {
+		tmpblock->size = tmpsize - XV_ALIGN;
+		set_flag(tmpblock, BLOCK_FREE);
+		clear_flag(tmpblock, PREV_FREE);
+
+		set_blockprev(tmpblock, *offset);
+		if (tmpblock->size >= XV_MIN_ALLOC_SIZE)
+			insert_block(pool, *page, tmpoffset, tmpblock);
+
+		if (tmpoffset + XV_ALIGN + tmpblock->size != PAGE_SIZE) {
+			tmpblock = BLOCK_NEXT(tmpblock);
+			set_blockprev(tmpblock, tmpoffset);
+		}
+	} else {
+		/* This block is exact fit */
+		if (tmpoffset != PAGE_SIZE)
+			clear_flag(tmpblock, PREV_FREE);
+	}
+
+	block->size = origsize;
+	clear_flag(block, BLOCK_FREE);
+
+	put_ptr_atomic(block, KM_USER0);
+	spin_unlock(&pool->lock);
+
+	*offset += XV_ALIGN;
+
+	return 0;
+}
+
+/*
+ * Free block identified with <page, offset>
+ */
+void xv_free(struct xv_pool *pool, struct page *page, u32 offset)
+{
+	void *page_start;
+	struct block_header *block, *tmpblock;
+
+	offset -= XV_ALIGN;
+
+	spin_lock(&pool->lock);
+
+	page_start = get_ptr_atomic(page, 0, KM_USER0);
+	block = (struct block_header *)((char *)page_start + offset);
+
+	/* Catch double free bugs */
+	BUG_ON(test_flag(block, BLOCK_FREE));
+
+	block->size = ALIGN(block->size, XV_ALIGN);
+
+	tmpblock = BLOCK_NEXT(block);
+	if (offset + block->size + XV_ALIGN == PAGE_SIZE)
+		tmpblock = NULL;
+
+	/* Merge next block if its free */
+	if (tmpblock && test_flag(tmpblock, BLOCK_FREE)) {
+		/*
+		 * Blocks smaller than XV_MIN_ALLOC_SIZE
+		 * are not inserted in any free list.
+		 */
+		if (tmpblock->size >= XV_MIN_ALLOC_SIZE) {
+			remove_block(pool, page,
+				    offset + block->size + XV_ALIGN, tmpblock,
+				    get_index_for_insert(tmpblock->size));
+		}
+		block->size += tmpblock->size + XV_ALIGN;
+	}
+
+	/* Merge previous block if its free */
+	if (test_flag(block, PREV_FREE)) {
+		tmpblock = (struct block_header *)((char *)(page_start) +
+						get_blockprev(block));
+		offset = offset - tmpblock->size - XV_ALIGN;
+
+		if (tmpblock->size >= XV_MIN_ALLOC_SIZE)
+			remove_block(pool, page, offset, tmpblock,
+				    get_index_for_insert(tmpblock->size));
+
+		tmpblock->size += block->size + XV_ALIGN;
+		block = tmpblock;
+	}
+
+	/* No used objects in this page. Free it. */
+	if (block->size == PAGE_SIZE - XV_ALIGN) {
+		put_ptr_atomic(page_start, KM_USER0);
+		spin_unlock(&pool->lock);
+
+		__free_page(page);
+		stat_dec(&pool->total_pages);
+		return;
+	}
+
+	set_flag(block, BLOCK_FREE);
+	if (block->size >= XV_MIN_ALLOC_SIZE)
+		insert_block(pool, page, offset, block);
+
+	if (offset + block->size + XV_ALIGN != PAGE_SIZE) {
+		tmpblock = BLOCK_NEXT(block);
+		set_flag(tmpblock, PREV_FREE);
+		set_blockprev(tmpblock, offset);
+	}
+
+	put_ptr_atomic(page_start, KM_USER0);
+	spin_unlock(&pool->lock);
+}
+
+u32 xv_get_object_size(void *obj)
+{
+	struct block_header *blk;
+
+	blk = (struct block_header *)((char *)(obj) - XV_ALIGN);
+	return blk->size;
+}
+
+/*
+ * Returns total memory used by allocator (userdata + metadata)
+ */
+u64 xv_get_total_size_bytes(struct xv_pool *pool)
+{
+	return pool->total_pages << PAGE_SHIFT;
+}
diff -u -r -N driver-master/compcache/sub-projects/allocators/xvmalloc-kmod/xvmalloc.h fulan-dvb-drivers-master/compcache/sub-projects/allocators/xvmalloc-kmod/xvmalloc.h
--- driver-master/compcache/sub-projects/allocators/xvmalloc-kmod/xvmalloc.h	1970-01-01 01:00:00.000000000 +0100
+++ fulan-dvb-drivers-master/compcache/sub-projects/allocators/xvmalloc-kmod/xvmalloc.h	2018-04-05 09:41:50.000000000 +0200
@@ -0,0 +1,30 @@
+/*
+ * xvmalloc memory allocator
+ *
+ * Copyright (C) 2008, 2009, 2010  Nitin Gupta
+ *
+ * This code is released using a dual license strategy: BSD/GPL
+ * You can choose the licence that better fits your requirements.
+ *
+ * Released under the terms of 3-clause BSD License
+ * Released under the terms of GNU General Public License Version 2.0
+ */
+
+#ifndef _XV_MALLOC_H_
+#define _XV_MALLOC_H_
+
+#include <linux/types.h>
+
+struct xv_pool;
+
+struct xv_pool *xv_create_pool(void);
+void xv_destroy_pool(struct xv_pool *pool);
+
+int xv_malloc(struct xv_pool *pool, u32 size, struct page **page,
+			u32 *offset, gfp_t flags);
+void xv_free(struct xv_pool *pool, struct page *page, u32 offset);
+
+u32 xv_get_object_size(void *obj);
+u64 xv_get_total_size_bytes(struct xv_pool *pool);
+
+#endif
diff -u -r -N driver-master/compcache/sub-projects/allocators/xvmalloc-kmod/xvmalloc_int.h fulan-dvb-drivers-master/compcache/sub-projects/allocators/xvmalloc-kmod/xvmalloc_int.h
--- driver-master/compcache/sub-projects/allocators/xvmalloc-kmod/xvmalloc_int.h	1970-01-01 01:00:00.000000000 +0100
+++ fulan-dvb-drivers-master/compcache/sub-projects/allocators/xvmalloc-kmod/xvmalloc_int.h	2018-05-16 23:58:31.454714147 +0200
@@ -0,0 +1,95 @@
+/*
+ * xvmalloc memory allocator
+ *
+ * Copyright (C) 2008, 2009, 2010  Nitin Gupta
+ *
+ * This code is released using a dual license strategy: BSD/GPL
+ * You can choose the licence that better fits your requirements.
+ *
+ * Released under the terms of 3-clause BSD License
+ * Released under the terms of GNU General Public License Version 2.0
+ */
+
+#ifndef _XV_MALLOC_INT_H_
+#define _XV_MALLOC_INT_H_
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+
+/* User configurable params */
+
+/* Must be power of two */
+#ifdef CONFIG_64BIT
+#define XV_ALIGN_SHIFT 3
+#else
+#define XV_ALIGN_SHIFT	2
+#endif
+#define XV_ALIGN	(1 << XV_ALIGN_SHIFT)
+#define XV_ALIGN_MASK	(XV_ALIGN - 1)
+
+/* This must be greater than sizeof(link_free) */
+#define XV_MIN_ALLOC_SIZE	32
+#define XV_MAX_ALLOC_SIZE	(PAGE_SIZE - XV_ALIGN)
+
+/*
+ * Free lists are separated by FL_DELTA bytes
+ * This value is 3 for 4k pages and 4 for 64k pages, for any
+ * other page size, a conservative (PAGE_SHIFT - 9) is used.
+ */
+#if PAGE_SHIFT == 16
+#define FL_DELTA_SHIFT 4
+#else
+#define FL_DELTA_SHIFT (PAGE_SHIFT - 9)
+#endif
+#define FL_DELTA	(1 << FL_DELTA_SHIFT)
+#define FL_DELTA_MASK	(FL_DELTA - 1)
+#define NUM_FREE_LISTS	((XV_MAX_ALLOC_SIZE - XV_MIN_ALLOC_SIZE) \
+				/ FL_DELTA + 1)
+
+#define MAX_FLI		DIV_ROUND_UP(NUM_FREE_LISTS, BITS_PER_LONG)
+
+/* End of user params */
+
+enum blockflags {
+	BLOCK_FREE,
+	PREV_FREE,
+	__NR_BLOCKFLAGS,
+};
+
+#define FLAGS_MASK	XV_ALIGN_MASK
+#define PREV_MASK	(~FLAGS_MASK)
+
+struct freelist_entry {
+	struct page *page;
+	u16 offset;
+	u16 pad;
+};
+
+struct link_free {
+	struct page *prev_page;
+	struct page *next_page;
+	u16 prev_offset;
+	u16 next_offset;
+};
+
+struct block_header {
+	union {
+		/* This common header must be XV_ALIGN bytes */
+		u8 common[XV_ALIGN];
+		struct {
+			u16 size;
+			u16 prev;
+		};
+	};
+	struct link_free link;
+};
+
+struct xv_pool {
+	ulong flbitmap;
+	ulong slbitmap[MAX_FLI];
+	u64 total_pages;	/* stats */
+	struct freelist_entry freelist[NUM_FREE_LISTS];
+	spinlock_t lock;
+};
+
+#endif
diff -u -r -N driver-master/compcache/zram_drv.c fulan-dvb-drivers-master/compcache/zram_drv.c
--- driver-master/compcache/zram_drv.c	1970-01-01 01:00:00.000000000 +0100
+++ fulan-dvb-drivers-master/compcache/zram_drv.c	2018-05-16 23:55:45.285582552 +0200
@@ -0,0 +1,865 @@
+/*
+ * Compressed RAM block device
+ *
+ * Copyright (C) 2008, 2009, 2010  Nitin Gupta
+ *
+ * This code is released using a dual license strategy: BSD/GPL
+ * You can choose the licence that better fits your requirements.
+ *
+ * Released under the terms of 3-clause BSD License
+ * Released under the terms of GNU General Public License Version 2.0
+ *
+ * Project home: http://compcache.googlecode.com
+ */
+
+#define KMSG_COMPONENT "zram"
+#define pr_fmt(fmt) KMSG_COMPONENT ": " fmt
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/bio.h>
+#include <linux/bitops.h>
+#include <linux/blkdev.h>
+#include <linux/buffer_head.h>
+#include <linux/device.h>
+#include <linux/genhd.h>
+#include <linux/highmem.h>
+#include <linux/slab.h>
+#include <linux/lzo.h>
+#include <linux/string.h>
+#include <linux/vmalloc.h>
+
+#include "zram_drv.h"
+
+/* Globals */
+static int zram_major;
+struct zram *devices;
+
+/* Module params (documentation at end) */
+unsigned int num_devices;
+
+static void zram_stat_inc(u32 *v)
+{
+	*v = *v + 1;
+}
+
+static void zram_stat_dec(u32 *v)
+{
+	*v = *v - 1;
+}
+
+static void zram_stat64_add(struct zram *zram, u64 *v, u64 inc)
+{
+	spin_lock(&zram->stat64_lock);
+	*v = *v + inc;
+	spin_unlock(&zram->stat64_lock);
+}
+
+static void zram_stat64_sub(struct zram *zram, u64 *v, u64 dec)
+{
+	spin_lock(&zram->stat64_lock);
+	*v = *v - dec;
+	spin_unlock(&zram->stat64_lock);
+}
+
+static void zram_stat64_inc(struct zram *zram, u64 *v)
+{
+	zram_stat64_add(zram, v, 1);
+}
+
+static int zram_test_flag(struct zram *zram, u32 index,
+			enum zram_pageflags flag)
+{
+	return zram->table[index].flags & BIT(flag);
+}
+
+static void zram_set_flag(struct zram *zram, u32 index,
+			enum zram_pageflags flag)
+{
+	zram->table[index].flags |= BIT(flag);
+}
+
+static void zram_clear_flag(struct zram *zram, u32 index,
+			enum zram_pageflags flag)
+{
+	zram->table[index].flags &= ~BIT(flag);
+}
+
+static int page_zero_filled(void *ptr)
+{
+	unsigned int pos;
+	unsigned long *page;
+
+	page = (unsigned long *)ptr;
+
+	for (pos = 0; pos != PAGE_SIZE / sizeof(*page); pos++) {
+		if (page[pos])
+			return 0;
+	}
+
+	return 1;
+}
+
+static void zram_set_disksize(struct zram *zram, size_t totalram_bytes)
+{
+	if (!zram->disksize) {
+		pr_info(
+		"disk size not provided. You can use disksize_kb module "
+		"param to specify size.\nUsing default: (%u%% of RAM).\n",
+		default_disksize_perc_ram
+		);
+		zram->disksize = default_disksize_perc_ram *
+					(totalram_bytes / 100);
+	}
+
+	if (zram->disksize > 2 * (totalram_bytes)) {
+		pr_info(
+		"There is little point creating a zram of greater than "
+		"twice the size of memory since we expect a 2:1 compression "
+		"ratio. Note that zram uses about 0.1%% of the size of "
+		"the disk when not in use so a huge zram is "
+		"wasteful.\n"
+		"\tMemory Size: %zu kB\n"
+		"\tSize you selected: %llu kB\n"
+		"Continuing anyway ...\n",
+		totalram_bytes >> 10, zram->disksize
+		);
+	}
+
+	zram->disksize &= PAGE_MASK;
+}
+
+static void zram_free_page(struct zram *zram, size_t index)
+{
+	u32 clen;
+	void *obj;
+
+	struct page *page = zram->table[index].page;
+	u32 offset = zram->table[index].offset;
+
+	if (unlikely(!page)) {
+		/*
+		 * No memory is allocated for zero filled pages.
+		 * Simply clear zero page flag.
+		 */
+		if (zram_test_flag(zram, index, ZRAM_ZERO)) {
+			zram_clear_flag(zram, index, ZRAM_ZERO);
+			zram_stat_dec(&zram->stats.pages_zero);
+		}
+		return;
+	}
+
+	if (unlikely(zram_test_flag(zram, index, ZRAM_UNCOMPRESSED))) {
+		clen = PAGE_SIZE;
+		__free_page(page);
+		zram_clear_flag(zram, index, ZRAM_UNCOMPRESSED);
+		zram_stat_dec(&zram->stats.pages_expand);
+		goto out;
+	}
+
+	obj = kmap_atomic(page, KM_USER0) + offset;
+	clen = xv_get_object_size(obj) - sizeof(struct zobj_header);
+	kunmap_atomic(obj, KM_USER0);
+
+	xv_free(zram->mem_pool, page, offset);
+	if (clen <= PAGE_SIZE / 2)
+		zram_stat_dec(&zram->stats.good_compress);
+
+out:
+	zram_stat64_sub(zram, &zram->stats.compr_size, clen);
+	zram_stat_dec(&zram->stats.pages_stored);
+
+	zram->table[index].page = NULL;
+	zram->table[index].offset = 0;
+}
+
+static void handle_zero_page(struct bio_vec *bvec)
+{
+	struct page *page = bvec->bv_page;
+	void *user_mem;
+
+	user_mem = kmap_atomic(page, KM_USER0);
+	memset(user_mem + bvec->bv_offset, 0, bvec->bv_len);
+	kunmap_atomic(user_mem, KM_USER0);
+
+	flush_dcache_page(page);
+}
+
+static void handle_uncompressed_page(struct zram *zram, struct bio_vec *bvec,
+				     u32 index, int offset)
+{
+	struct page *page = bvec->bv_page;
+	unsigned char *user_mem, *cmem;
+
+	user_mem = kmap_atomic(page, KM_USER0);
+	cmem = kmap_atomic(zram->table[index].page, KM_USER1);
+
+	memcpy(user_mem + bvec->bv_offset, cmem + offset, bvec->bv_len);
+	kunmap_atomic(cmem, KM_USER1);
+	kunmap_atomic(user_mem, KM_USER0);
+
+	flush_dcache_page(page);
+}
+
+static inline int is_partial_io(struct bio_vec *bvec)
+{
+	return bvec->bv_len != PAGE_SIZE;
+}
+
+static int zram_bvec_read(struct zram *zram, struct bio_vec *bvec,
+			  u32 index, int offset, struct bio *bio)
+{
+	int ret;
+	size_t clen;
+	struct page *page;
+	struct zobj_header *zheader;
+	unsigned char *user_mem, *cmem, *uncmem = NULL;
+
+	page = bvec->bv_page;
+
+	if (zram_test_flag(zram, index, ZRAM_ZERO)) {
+		handle_zero_page(bvec);
+		return 0;
+	}
+
+	/* Requested page is not present in compressed area */
+	if (unlikely(!zram->table[index].page)) {
+		pr_debug("Read before write: sector=%lu, size=%u",
+			 (ulong)(bio->bi_sector), bio->bi_size);
+		handle_zero_page(bvec);
+		return 0;
+	}
+
+	/* Page is stored uncompressed since it's incompressible */
+	if (unlikely(zram_test_flag(zram, index, ZRAM_UNCOMPRESSED))) {
+		handle_uncompressed_page(zram, bvec, index, offset);
+		return 0;
+	}
+
+	if (is_partial_io(bvec)) {
+		/* Use  a temporary buffer to decompress the page */
+		uncmem = kmalloc(PAGE_SIZE, GFP_KERNEL);
+		if (!uncmem) {
+			pr_info("Error allocating temp memory!\n");
+			return -ENOMEM;
+		}
+	}
+
+	user_mem = kmap_atomic(page, KM_USER0);
+	if (!is_partial_io(bvec))
+		uncmem = user_mem;
+	clen = PAGE_SIZE;
+
+	cmem = kmap_atomic(zram->table[index].page, KM_USER1) +
+		zram->table[index].offset;
+
+	ret = lzo1x_decompress_safe(cmem + sizeof(*zheader),
+				    xv_get_object_size(cmem) - sizeof(*zheader),
+				    uncmem, &clen);
+
+	if (is_partial_io(bvec)) {
+		memcpy(user_mem + bvec->bv_offset, uncmem + offset,
+		       bvec->bv_len);
+		kfree(uncmem);
+	}
+
+	kunmap_atomic(cmem, KM_USER1);
+	kunmap_atomic(user_mem, KM_USER0);
+
+	/* Should NEVER happen. Return bio error if it does. */
+	if (unlikely(ret != LZO_E_OK)) {
+		pr_err("Decompression failed! err=%d, page=%u\n", ret, index);
+		zram_stat64_inc(zram, &zram->stats.failed_reads);
+		return ret;
+	}
+
+	flush_dcache_page(page);
+
+	return 0;
+}
+
+static int zram_read_before_write(struct zram *zram, char *mem, u32 index)
+{
+	int ret;
+	size_t clen = PAGE_SIZE;
+	struct zobj_header *zheader;
+	unsigned char *cmem;
+
+	if (zram_test_flag(zram, index, ZRAM_ZERO) ||
+	    !zram->table[index].page) {
+		memset(mem, 0, PAGE_SIZE);
+		return 0;
+	}
+
+	cmem = kmap_atomic(zram->table[index].page, KM_USER0) +
+		zram->table[index].offset;
+
+	/* Page is stored uncompressed since it's incompressible */
+	if (unlikely(zram_test_flag(zram, index, ZRAM_UNCOMPRESSED))) {
+		memcpy(mem, cmem, PAGE_SIZE);
+		kunmap_atomic(cmem, KM_USER0);
+		return 0;
+	}
+
+	ret = lzo1x_decompress_safe(cmem + sizeof(*zheader),
+				    xv_get_object_size(cmem) - sizeof(*zheader),
+				    mem, &clen);
+	kunmap_atomic(cmem, KM_USER0);
+
+	/* Should NEVER happen. Return bio error if it does. */
+	if (unlikely(ret != LZO_E_OK)) {
+		pr_err("Decompression failed! err=%d, page=%u\n", ret, index);
+		zram_stat64_inc(zram, &zram->stats.failed_reads);
+		return ret;
+	}
+
+	return 0;
+}
+
+static int zram_bvec_write(struct zram *zram, struct bio_vec *bvec, u32 index,
+			   int offset)
+{
+	int ret;
+	u32 store_offset;
+	size_t clen;
+	struct zobj_header *zheader;
+	struct page *page, *page_store;
+	unsigned char *user_mem, *cmem, *src, *uncmem = NULL;
+
+	page = bvec->bv_page;
+	src = zram->compress_buffer;
+
+	if (is_partial_io(bvec)) {
+		/*
+		 * This is a partial IO. We need to read the full page
+		 * before to write the changes.
+		 */
+		uncmem = kmalloc(PAGE_SIZE, GFP_KERNEL);
+		if (!uncmem) {
+			pr_info("Error allocating temp memory!\n");
+			ret = -ENOMEM;
+			goto out;
+		}
+		ret = zram_read_before_write(zram, uncmem, index);
+		if (ret) {
+			kfree(uncmem);
+			goto out;
+		}
+	}
+
+	/*
+	 * System overwrites unused sectors. Free memory associated
+	 * with this sector now.
+	 */
+	if (zram->table[index].page ||
+	    zram_test_flag(zram, index, ZRAM_ZERO))
+		zram_free_page(zram, index);
+
+	user_mem = kmap_atomic(page, KM_USER0);
+
+	if (is_partial_io(bvec))
+		memcpy(uncmem + offset, user_mem + bvec->bv_offset,
+		       bvec->bv_len);
+	else
+		uncmem = user_mem;
+
+	if (page_zero_filled(uncmem)) {
+		kunmap_atomic(user_mem, KM_USER0);
+		if (is_partial_io(bvec))
+			kfree(uncmem);
+		zram_stat_inc(&zram->stats.pages_zero);
+		zram_set_flag(zram, index, ZRAM_ZERO);
+		ret = 0;
+		goto out;
+	}
+
+	ret = lzo1x_1_compress(uncmem, PAGE_SIZE, src, &clen,
+			       zram->compress_workmem);
+
+	kunmap_atomic(user_mem, KM_USER0);
+	if (is_partial_io(bvec))
+			kfree(uncmem);
+
+	if (unlikely(ret != LZO_E_OK)) {
+		pr_err("Compression failed! err=%d\n", ret);
+		goto out;
+	}
+
+	/*
+	 * Page is incompressible. Store it as-is (uncompressed)
+	 * since we do not want to return too many disk write
+	 * errors which has side effect of hanging the system.
+	 */
+	if (unlikely(clen > max_zpage_size)) {
+		clen = PAGE_SIZE;
+		page_store = alloc_page(GFP_NOIO | __GFP_HIGHMEM);
+		if (unlikely(!page_store)) {
+			pr_info("Error allocating memory for "
+				"incompressible page: %u\n", index);
+			ret = -ENOMEM;
+			goto out;
+		}
+
+		store_offset = 0;
+		zram_set_flag(zram, index, ZRAM_UNCOMPRESSED);
+		zram_stat_inc(&zram->stats.pages_expand);
+		zram->table[index].page = page_store;
+		src = kmap_atomic(page, KM_USER0);
+		goto memstore;
+	}
+
+	if (xv_malloc(zram->mem_pool, clen + sizeof(*zheader),
+		      &zram->table[index].page, &store_offset,
+		      GFP_NOIO | __GFP_HIGHMEM)) {
+		pr_info("Error allocating memory for compressed "
+			"page: %u, size=%zu\n", index, clen);
+		ret = -ENOMEM;
+		goto out;
+	}
+
+memstore:
+	zram->table[index].offset = store_offset;
+
+	cmem = kmap_atomic(zram->table[index].page, KM_USER1) +
+		zram->table[index].offset;
+
+#if 0
+	/* Back-reference needed for memory defragmentation */
+	if (!zram_test_flag(zram, index, ZRAM_UNCOMPRESSED)) {
+		zheader = (struct zobj_header *)cmem;
+		zheader->table_idx = index;
+		cmem += sizeof(*zheader);
+	}
+#endif
+
+	memcpy(cmem, src, clen);
+
+	kunmap_atomic(cmem, KM_USER1);
+	if (unlikely(zram_test_flag(zram, index, ZRAM_UNCOMPRESSED)))
+		kunmap_atomic(src, KM_USER0);
+
+	/* Update stats */
+	zram_stat64_add(zram, &zram->stats.compr_size, clen);
+	zram_stat_inc(&zram->stats.pages_stored);
+	if (clen <= PAGE_SIZE / 2)
+		zram_stat_inc(&zram->stats.good_compress);
+
+	return 0;
+
+out:
+	if (ret)
+		zram_stat64_inc(zram, &zram->stats.failed_writes);
+	return ret;
+}
+
+static int zram_bvec_rw(struct zram *zram, struct bio_vec *bvec, u32 index,
+			int offset, struct bio *bio, int rw)
+{
+	int ret;
+
+	if (rw == READ) {
+		down_read(&zram->lock);
+		ret = zram_bvec_read(zram, bvec, index, offset, bio);
+		up_read(&zram->lock);
+	} else {
+		down_write(&zram->lock);
+		ret = zram_bvec_write(zram, bvec, index, offset);
+		up_write(&zram->lock);
+	}
+
+	return ret;
+}
+
+static void update_position(u32 *index, int *offset, struct bio_vec *bvec)
+{
+	if (*offset + bvec->bv_len >= PAGE_SIZE)
+		(*index)++;
+	*offset = (*offset + bvec->bv_len) % PAGE_SIZE;
+}
+
+static void __zram_make_request(struct zram *zram, struct bio *bio, int rw)
+{
+	int i, offset;
+	u32 index;
+	struct bio_vec *bvec;
+
+	switch (rw) {
+	case READ:
+		zram_stat64_inc(zram, &zram->stats.num_reads);
+		break;
+	case WRITE:
+		zram_stat64_inc(zram, &zram->stats.num_writes);
+		break;
+	}
+
+	index = bio->bi_sector >> SECTORS_PER_PAGE_SHIFT;
+	offset = (bio->bi_sector & (SECTORS_PER_PAGE - 1)) << SECTOR_SHIFT;
+
+	bio_for_each_segment(bvec, bio, i) {
+		int max_transfer_size = PAGE_SIZE - offset;
+
+		if (bvec->bv_len > max_transfer_size) {
+			/*
+			 * zram_bvec_rw() can only make operation on a single
+			 * zram page. Split the bio vector.
+			 */
+			struct bio_vec bv;
+
+			bv.bv_page = bvec->bv_page;
+			bv.bv_len = max_transfer_size;
+			bv.bv_offset = bvec->bv_offset;
+
+			if (zram_bvec_rw(zram, &bv, index, offset, bio, rw) < 0)
+				goto out;
+
+			bv.bv_len = bvec->bv_len - max_transfer_size;
+			bv.bv_offset += max_transfer_size;
+			if (zram_bvec_rw(zram, &bv, index+1, 0, bio, rw) < 0)
+				goto out;
+		} else
+			if (zram_bvec_rw(zram, bvec, index, offset, bio, rw)
+			    < 0)
+				goto out;
+
+		update_position(&index, &offset, bvec);
+	}
+
+	set_bit(BIO_UPTODATE, &bio->bi_flags);
+	bio_endio(bio, 0);
+	return;
+
+out:
+	bio_io_error(bio);
+}
+
+/*
+ * Check if request is within bounds and aligned on zram logical blocks.
+ */
+static inline int valid_io_request(struct zram *zram, struct bio *bio)
+{
+	if (unlikely(
+		(bio->bi_sector >= (zram->disksize >> SECTOR_SHIFT)) ||
+		(bio->bi_sector & (ZRAM_SECTOR_PER_LOGICAL_BLOCK - 1)) ||
+		(bio->bi_size & (ZRAM_LOGICAL_BLOCK_SIZE - 1)))) {
+
+		return 0;
+	}
+
+	/* I/O request is valid */
+	return 1;
+}
+
+/*
+ * Handler function for all zram I/O requests.
+ */
+static int zram_make_request(struct request_queue *queue, struct bio *bio)
+{
+	struct zram *zram = queue->queuedata;
+
+	if (unlikely(!zram->init_done) && zram_init_device(zram))
+		goto error;
+
+	down_read(&zram->init_lock);
+	if (unlikely(!zram->init_done))
+		goto error_unlock;
+
+	if (!valid_io_request(zram, bio)) {
+		zram_stat64_inc(zram, &zram->stats.invalid_io);
+		goto error_unlock;
+	}
+
+	__zram_make_request(zram, bio, bio_data_dir(bio));
+	up_read(&zram->init_lock);
+
+	return 0;
+
+error_unlock:
+	up_read(&zram->init_lock);
+error:
+	bio_io_error(bio);
+	return 0;
+}
+
+void __zram_reset_device(struct zram *zram)
+{
+	size_t index;
+
+	zram->init_done = 0;
+
+	/* Free various per-device buffers */
+	kfree(zram->compress_workmem);
+	free_pages((unsigned long)zram->compress_buffer, 1);
+
+	zram->compress_workmem = NULL;
+	zram->compress_buffer = NULL;
+
+	/* Free all pages that are still in this zram device */
+	for (index = 0; index < zram->disksize >> PAGE_SHIFT; index++) {
+		struct page *page;
+		u16 offset;
+
+		page = zram->table[index].page;
+		offset = zram->table[index].offset;
+
+		if (!page)
+			continue;
+
+		if (unlikely(zram_test_flag(zram, index, ZRAM_UNCOMPRESSED)))
+			__free_page(page);
+		else
+			xv_free(zram->mem_pool, page, offset);
+	}
+
+	vfree(zram->table);
+	zram->table = NULL;
+
+	xv_destroy_pool(zram->mem_pool);
+	zram->mem_pool = NULL;
+
+	/* Reset stats */
+	memset(&zram->stats, 0, sizeof(zram->stats));
+
+	zram->disksize = 0;
+}
+
+void zram_reset_device(struct zram *zram)
+{
+	down_write(&zram->init_lock);
+	__zram_reset_device(zram);
+	up_write(&zram->init_lock);
+}
+
+int zram_init_device(struct zram *zram)
+{
+	int ret;
+	size_t num_pages;
+
+	down_write(&zram->init_lock);
+
+	if (zram->init_done) {
+		up_write(&zram->init_lock);
+		return 0;
+	}
+
+	zram_set_disksize(zram, totalram_pages << PAGE_SHIFT);
+
+	zram->compress_workmem = kzalloc(LZO1X_MEM_COMPRESS, GFP_KERNEL);
+	if (!zram->compress_workmem) {
+		pr_err("Error allocating compressor working memory!\n");
+		ret = -ENOMEM;
+		goto fail_no_table;
+	}
+
+	zram->compress_buffer = (void *)__get_free_pages(__GFP_ZERO, 1);
+	if (!zram->compress_buffer) {
+		pr_err("Error allocating compressor buffer space\n");
+		ret = -ENOMEM;
+		goto fail_no_table;
+	}
+
+	num_pages = zram->disksize >> PAGE_SHIFT;
+	zram->table = vmalloc(num_pages * sizeof(*zram->table));
+	if (!zram->table) {
+		pr_err("Error allocating zram address table\n");
+		ret = -ENOMEM;
+		goto fail_no_table;
+	}
+	memset(zram->table, 0, num_pages * sizeof(*zram->table));
+
+	set_capacity(zram->disk, zram->disksize >> SECTOR_SHIFT);
+
+	/* zram devices sort of resembles non-rotational disks */
+	queue_flag_set_unlocked(QUEUE_FLAG_NONROT, zram->disk->queue);
+
+	zram->mem_pool = xv_create_pool();
+	if (!zram->mem_pool) {
+		pr_err("Error creating memory pool\n");
+		ret = -ENOMEM;
+		goto fail;
+	}
+
+	zram->init_done = 1;
+	up_write(&zram->init_lock);
+
+	pr_debug("Initialization done!\n");
+	return 0;
+
+fail_no_table:
+	/* To prevent accessing table entries during cleanup */
+	zram->disksize = 0;
+fail:
+	__zram_reset_device(zram);
+	up_write(&zram->init_lock);
+	pr_err("Initialization failed: err=%d\n", ret);
+	return ret;
+}
+
+void zram_slot_free_notify(struct block_device *bdev, unsigned long index)
+{
+	struct zram *zram;
+
+	zram = bdev->bd_disk->private_data;
+	zram_free_page(zram, index);
+	zram_stat64_inc(zram, &zram->stats.notify_free);
+}
+
+static const struct block_device_operations zram_devops = {
+	.swap_slot_free_notify = zram_slot_free_notify,
+	.owner = THIS_MODULE
+};
+
+static int create_device(struct zram *zram, int device_id)
+{
+	int ret = 0;
+
+	init_rwsem(&zram->lock);
+	init_rwsem(&zram->init_lock);
+	spin_lock_init(&zram->stat64_lock);
+
+	zram->queue = blk_alloc_queue(GFP_KERNEL);
+	if (!zram->queue) {
+		pr_err("Error allocating disk queue for device %d\n",
+			device_id);
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	blk_queue_make_request(zram->queue, zram_make_request);
+	zram->queue->queuedata = zram;
+
+	 /* gendisk structure */
+	zram->disk = alloc_disk(1);
+	if (!zram->disk) {
+		blk_cleanup_queue(zram->queue);
+		pr_warning("Error allocating disk structure for device %d\n",
+			device_id);
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	zram->disk->major = zram_major;
+	zram->disk->first_minor = device_id;
+	zram->disk->fops = &zram_devops;
+	zram->disk->queue = zram->queue;
+	zram->disk->private_data = zram;
+	snprintf(zram->disk->disk_name, 16, "zram%d", device_id);
+
+	/* Actual capacity set using syfs (/sys/block/zram<id>/disksize */
+	set_capacity(zram->disk, 0);
+
+	/*
+	 * To ensure that we always get PAGE_SIZE aligned
+	 * and n*PAGE_SIZED sized I/O requests.
+	 */
+	blk_queue_physical_block_size(zram->disk->queue, PAGE_SIZE);
+	blk_queue_logical_block_size(zram->disk->queue,
+					ZRAM_LOGICAL_BLOCK_SIZE);
+	blk_queue_io_min(zram->disk->queue, PAGE_SIZE);
+	blk_queue_io_opt(zram->disk->queue, PAGE_SIZE);
+
+	add_disk(zram->disk);
+
+	ret = sysfs_create_group(&disk_to_dev(zram->disk)->kobj,
+				&zram_disk_attr_group);
+	if (ret < 0) {
+		pr_warning("Error creating sysfs group");
+		goto out;
+	}
+
+	zram->init_done = 0;
+
+out:
+	return ret;
+}
+
+static void destroy_device(struct zram *zram)
+{
+	sysfs_remove_group(&disk_to_dev(zram->disk)->kobj,
+			&zram_disk_attr_group);
+
+	if (zram->disk) {
+		del_gendisk(zram->disk);
+		put_disk(zram->disk);
+	}
+
+	if (zram->queue)
+		blk_cleanup_queue(zram->queue);
+}
+
+static int __init zram_init(void)
+{
+	int ret, dev_id;
+
+	if (num_devices > max_num_devices) {
+		pr_warning("Invalid value for num_devices: %u\n",
+				num_devices);
+		ret = -EINVAL;
+		goto out;
+	}
+
+	zram_major = register_blkdev(0, "zram");
+	if (zram_major <= 0) {
+		pr_warning("Unable to get major number\n");
+		ret = -EBUSY;
+		goto out;
+	}
+
+	if (!num_devices) {
+		pr_info("num_devices not specified. Using default: 1\n");
+		num_devices = 1;
+	}
+
+	/* Allocate the device array and initialize each one */
+	pr_info("Creating %u devices ...\n", num_devices);
+	devices = kzalloc(num_devices * sizeof(struct zram), GFP_KERNEL);
+	if (!devices) {
+		ret = -ENOMEM;
+		goto unregister;
+	}
+
+	for (dev_id = 0; dev_id < num_devices; dev_id++) {
+		ret = create_device(&devices[dev_id], dev_id);
+		if (ret)
+			goto free_devices;
+	}
+
+	return 0;
+
+free_devices:
+	while (dev_id)
+		destroy_device(&devices[--dev_id]);
+	kfree(devices);
+unregister:
+	unregister_blkdev(zram_major, "zram");
+out:
+	return ret;
+}
+
+static void __exit zram_exit(void)
+{
+	int i;
+	struct zram *zram;
+
+	for (i = 0; i < num_devices; i++) {
+		zram = &devices[i];
+
+		destroy_device(zram);
+		if (zram->init_done)
+			zram_reset_device(zram);
+	}
+
+	unregister_blkdev(zram_major, "zram");
+
+	kfree(devices);
+	pr_debug("Cleanup done!\n");
+}
+
+module_param(num_devices, uint, 0);
+MODULE_PARM_DESC(num_devices, "Number of zram devices");
+
+module_init(zram_init);
+module_exit(zram_exit);
+
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_AUTHOR("Nitin Gupta <ngupta@vflare.org>");
+MODULE_DESCRIPTION("Compressed RAM Block Device");
diff -u -r -N driver-master/compcache/zram_drv.h fulan-dvb-drivers-master/compcache/zram_drv.h
--- driver-master/compcache/zram_drv.h	1970-01-01 01:00:00.000000000 +0100
+++ fulan-dvb-drivers-master/compcache/zram_drv.h	2018-05-16 23:52:56.876614259 +0200
@@ -0,0 +1,135 @@
+/*
+ * Compressed RAM block device
+ *
+ * Copyright (C) 2008, 2009, 2010  Nitin Gupta
+ *
+ * This code is released using a dual license strategy: BSD/GPL
+ * You can choose the licence that better fits your requirements.
+ *
+ * Released under the terms of 3-clause BSD License
+ * Released under the terms of GNU General Public License Version 2.0
+ *
+ * Project home: http://compcache.googlecode.com
+ */
+
+#ifndef _ZRAM_DRV_H_
+#define _ZRAM_DRV_H_
+
+#include <linux/spinlock.h>
+#include <linux/mutex.h>
+
+#include "sub-projects/allocators/xvmalloc-kmod/xvmalloc.h"
+
+/*
+ * Some arbitrary value. This is just to catch
+ * invalid value for num_devices module parameter.
+ */
+static const unsigned max_num_devices = 32;
+
+/*
+ * Stored at beginning of each compressed object.
+ *
+ * It stores back-reference to table entry which points to this
+ * object. This is required to support memory defragmentation.
+ */
+struct zobj_header {
+#if 0
+	u32 table_idx;
+#endif
+};
+
+/*-- Configurable parameters */
+
+/* Default zram disk size: 25% of total RAM */
+static const unsigned default_disksize_perc_ram = 25;
+
+/*
+ * Pages that compress to size greater than this are stored
+ * uncompressed in memory.
+ */
+static const unsigned max_zpage_size = PAGE_SIZE / 4 * 3;
+
+/*
+ * NOTE: max_zpage_size must be less than or equal to:
+ *   XV_MAX_ALLOC_SIZE - sizeof(struct zobj_header)
+ * otherwise, xv_malloc() would always return failure.
+ */
+
+/*-- End of configurable params */
+
+#define SECTOR_SHIFT		9
+#define SECTOR_SIZE		(1 << SECTOR_SHIFT)
+#define SECTORS_PER_PAGE_SHIFT	(PAGE_SHIFT - SECTOR_SHIFT)
+#define SECTORS_PER_PAGE	(1 << SECTORS_PER_PAGE_SHIFT)
+#define ZRAM_LOGICAL_BLOCK_SHIFT 12
+#define ZRAM_LOGICAL_BLOCK_SIZE	(1 << ZRAM_LOGICAL_BLOCK_SHIFT)
+#define ZRAM_SECTOR_PER_LOGICAL_BLOCK	\
+	(1 << (ZRAM_LOGICAL_BLOCK_SHIFT - SECTOR_SHIFT))
+
+/* Flags for zram pages (table[page_no].flags) */
+enum zram_pageflags {
+	/* Page is stored uncompressed */
+	ZRAM_UNCOMPRESSED,
+
+	/* Page consists entirely of zeros */
+	ZRAM_ZERO,
+
+	__NR_ZRAM_PAGEFLAGS,
+};
+
+/*-- Data structures */
+
+/* Allocated for each disk page */
+struct table {
+	struct page *page;
+	u16 offset;
+	u8 count;	/* object ref count (not yet used) */
+	u8 flags;
+} __attribute__((aligned(4)));
+
+struct zram_stats {
+	u64 compr_size;		/* compressed size of pages stored */
+	u64 num_reads;		/* failed + successful */
+	u64 num_writes;		/* --do-- */
+	u64 failed_reads;	/* should NEVER! happen */
+	u64 failed_writes;	/* can happen when memory is too low */
+	u64 invalid_io;		/* non-page-aligned I/O requests */
+	u64 notify_free;	/* no. of swap slot free notifications */
+	u32 pages_zero;		/* no. of zero filled pages */
+	u32 pages_stored;	/* no. of pages currently stored */
+	u32 good_compress;	/* % of pages with compression ratio<=50% */
+	u32 pages_expand;	/* % of incompressible pages */
+};
+
+struct zram {
+	struct xv_pool *mem_pool;
+	void *compress_workmem;
+	void *compress_buffer;
+	struct table *table;
+	spinlock_t stat64_lock;	/* protect 64-bit stats */
+	struct rw_semaphore lock; /* protect compression buffers and table
+				   * against concurrent read and writes */
+	struct request_queue *queue;
+	struct gendisk *disk;
+	int init_done;
+	/* Prevent concurrent execution of device init, reset and R/W request */
+	struct rw_semaphore init_lock;
+	/*
+	 * This is the limit on amount of *uncompressed* worth of data
+	 * we can store in a disk.
+	 */
+	u64 disksize;	/* bytes */
+
+	struct zram_stats stats;
+};
+
+extern struct zram *devices;
+extern unsigned int num_devices;
+#ifdef CONFIG_SYSFS
+extern struct attribute_group zram_disk_attr_group;
+#endif
+
+extern int zram_init_device(struct zram *zram);
+extern void __zram_reset_device(struct zram *zram);
+
+#endif
diff -u -r -N driver-master/compcache/zram_sysfs.c fulan-dvb-drivers-master/compcache/zram_sysfs.c
--- driver-master/compcache/zram_sysfs.c	1970-01-01 01:00:00.000000000 +0100
+++ fulan-dvb-drivers-master/compcache/zram_sysfs.c	2018-05-16 23:52:19.704431230 +0200
@@ -0,0 +1,227 @@
+/*
+ * Compressed RAM block device
+ *
+ * Copyright (C) 2008, 2009, 2010  Nitin Gupta
+ *
+ * This code is released using a dual license strategy: BSD/GPL
+ * You can choose the licence that better fits your requirements.
+ *
+ * Released under the terms of 3-clause BSD License
+ * Released under the terms of GNU General Public License Version 2.0
+ *
+ * Project home: http://compcache.googlecode.com/
+ */
+
+#include <linux/device.h>
+#include <linux/genhd.h>
+#include <linux/mm.h>
+
+#include "zram_drv.h"
+
+static u64 zram_stat64_read(struct zram *zram, u64 *v)
+{
+	u64 val;
+
+	spin_lock(&zram->stat64_lock);
+	val = *v;
+	spin_unlock(&zram->stat64_lock);
+
+	return val;
+}
+
+static struct zram *dev_to_zram(struct device *dev)
+{
+	int i;
+	struct zram *zram = NULL;
+
+	for (i = 0; i < num_devices; i++) {
+		zram = &devices[i];
+		if (disk_to_dev(zram->disk) == dev)
+			break;
+	}
+
+	return zram;
+}
+
+static ssize_t disksize_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct zram *zram = dev_to_zram(dev);
+
+	return sprintf(buf, "%llu\n", zram->disksize);
+}
+
+static ssize_t disksize_store(struct device *dev,
+		struct device_attribute *attr, const char *buf, size_t len)
+{
+	int ret;
+	u64 disksize;
+	struct zram *zram = dev_to_zram(dev);
+
+	ret = strict_strtoull(buf, 10, &disksize);
+	if (ret)
+		return ret;
+
+	down_write(&zram->init_lock);
+	if (zram->init_done) {
+		up_write(&zram->init_lock);
+		pr_info("Cannot change disksize for initialized device\n");
+		return -EBUSY;
+	}
+
+	zram->disksize = PAGE_ALIGN(disksize);
+	set_capacity(zram->disk, zram->disksize >> SECTOR_SHIFT);
+	up_write(&zram->init_lock);
+
+	return len;
+}
+
+static ssize_t initstate_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct zram *zram = dev_to_zram(dev);
+
+	return sprintf(buf, "%u\n", zram->init_done);
+}
+
+static ssize_t reset_store(struct device *dev,
+		struct device_attribute *attr, const char *buf, size_t len)
+{
+	int ret;
+	unsigned long do_reset;
+	struct zram *zram;
+	struct block_device *bdev;
+
+	zram = dev_to_zram(dev);
+	bdev = bdget_disk(zram->disk, 0);
+
+	/* Do not reset an active device! */
+	if (bdev->bd_holders)
+		return -EBUSY;
+
+	ret = strict_strtoul(buf, 10, &do_reset);
+	if (ret)
+		return ret;
+
+	if (!do_reset)
+		return -EINVAL;
+
+	/* Make sure all pending I/O is finished */
+	if (bdev)
+		fsync_bdev(bdev);
+
+	down_write(&zram->init_lock);
+	if (zram->init_done)
+		__zram_reset_device(zram);
+	up_write(&zram->init_lock);
+
+	return len;
+}
+
+static ssize_t num_reads_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct zram *zram = dev_to_zram(dev);
+
+	return sprintf(buf, "%llu\n",
+		zram_stat64_read(zram, &zram->stats.num_reads));
+}
+
+static ssize_t num_writes_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct zram *zram = dev_to_zram(dev);
+
+	return sprintf(buf, "%llu\n",
+		zram_stat64_read(zram, &zram->stats.num_writes));
+}
+
+static ssize_t invalid_io_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct zram *zram = dev_to_zram(dev);
+
+	return sprintf(buf, "%llu\n",
+		zram_stat64_read(zram, &zram->stats.invalid_io));
+}
+
+static ssize_t notify_free_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct zram *zram = dev_to_zram(dev);
+
+	return sprintf(buf, "%llu\n",
+		zram_stat64_read(zram, &zram->stats.notify_free));
+}
+
+static ssize_t zero_pages_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct zram *zram = dev_to_zram(dev);
+
+	return sprintf(buf, "%u\n", zram->stats.pages_zero);
+}
+
+static ssize_t orig_data_size_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct zram *zram = dev_to_zram(dev);
+
+	return sprintf(buf, "%llu\n",
+		(u64)(zram->stats.pages_stored) << PAGE_SHIFT);
+}
+
+static ssize_t compr_data_size_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct zram *zram = dev_to_zram(dev);
+
+	return sprintf(buf, "%llu\n",
+		zram_stat64_read(zram, &zram->stats.compr_size));
+}
+
+static ssize_t mem_used_total_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	u64 val = 0;
+	struct zram *zram = dev_to_zram(dev);
+
+	if (zram->init_done) {
+		val = xv_get_total_size_bytes(zram->mem_pool) +
+			((u64)(zram->stats.pages_expand) << PAGE_SHIFT);
+	}
+
+	return sprintf(buf, "%llu\n", val);
+}
+
+static DEVICE_ATTR(disksize, S_IRUGO | S_IWUSR,
+		disksize_show, disksize_store);
+static DEVICE_ATTR(initstate, S_IRUGO, initstate_show, NULL);
+static DEVICE_ATTR(reset, S_IWUSR, NULL, reset_store);
+static DEVICE_ATTR(num_reads, S_IRUGO, num_reads_show, NULL);
+static DEVICE_ATTR(num_writes, S_IRUGO, num_writes_show, NULL);
+static DEVICE_ATTR(invalid_io, S_IRUGO, invalid_io_show, NULL);
+static DEVICE_ATTR(notify_free, S_IRUGO, notify_free_show, NULL);
+static DEVICE_ATTR(zero_pages, S_IRUGO, zero_pages_show, NULL);
+static DEVICE_ATTR(orig_data_size, S_IRUGO, orig_data_size_show, NULL);
+static DEVICE_ATTR(compr_data_size, S_IRUGO, compr_data_size_show, NULL);
+static DEVICE_ATTR(mem_used_total, S_IRUGO, mem_used_total_show, NULL);
+
+static struct attribute *zram_disk_attrs[] = {
+	&dev_attr_disksize.attr,
+	&dev_attr_initstate.attr,
+	&dev_attr_reset.attr,
+	&dev_attr_num_reads.attr,
+	&dev_attr_num_writes.attr,
+	&dev_attr_invalid_io.attr,
+	&dev_attr_notify_free.attr,
+	&dev_attr_zero_pages.attr,
+	&dev_attr_orig_data_size.attr,
+	&dev_attr_compr_data_size.attr,
+	&dev_attr_mem_used_total.attr,
+	NULL,
+};
+
+struct attribute_group zram_disk_attr_group = {
+	.attrs = zram_disk_attrs,
+};
